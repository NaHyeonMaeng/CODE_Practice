{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN475XFUWAHqcZ5SSHZlVM4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NaHyeonMaeng/CODE_Practice/blob/main/Seq2Seq_%EB%AA%A8%EB%8D%B8%EB%A1%9C_%EC%B1%97%EB%B4%87_%EA%B5%AC%ED%98%84.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "_ShKGGqvKQib",
        "outputId": "55865a99-35cc-4020-abde-fabf767d1668"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 Q            A  label\n",
              "0           12시 땡!   하루가 또 가네요.      0\n",
              "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
              "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
              "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
              "4          PPL 심하네   눈살이 찌푸려지죠.      0"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-e8b55135-7d7a-45a9-a89c-a31d1ffc6c96\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Q</th>\n",
              "      <th>A</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>12시 땡!</td>\n",
              "      <td>하루가 또 가네요.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1지망 학교 떨어졌어</td>\n",
              "      <td>위로해 드립니다.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3박4일 놀러가고 싶다</td>\n",
              "      <td>여행은 언제나 좋죠.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3박4일 정도 놀러가고 싶다</td>\n",
              "      <td>여행은 언제나 좋죠.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>PPL 심하네</td>\n",
              "      <td>눈살이 찌푸려지죠.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e8b55135-7d7a-45a9-a89c-a31d1ffc6c96')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-126a6e27-e862-4720-9e11-66fadfecbbcc\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-126a6e27-e862-4720-9e11-66fadfecbbcc')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-126a6e27-e862-4720-9e11-66fadfecbbcc button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e8b55135-7d7a-45a9-a89c-a31d1ffc6c96 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e8b55135-7d7a-45a9-a89c-a31d1ffc6c96');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "corpus = pd.read_csv('https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv')\n",
        "corpus.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 챗봇 데이터 샘플 확인\n",
        "display(corpus['Q'].head()) #질문\n",
        "print('==='*10)\n",
        "display(corpus['A'].head()) #답변"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "id": "mc7g8itxK95k",
        "outputId": "f794b0d1-18d9-4bc1-cf4f-d53d72e8c720"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0             12시 땡!\n",
              "1        1지망 학교 떨어졌어\n",
              "2       3박4일 놀러가고 싶다\n",
              "3    3박4일 정도 놀러가고 싶다\n",
              "4            PPL 심하네\n",
              "Name: Q, dtype: object"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==============================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0     하루가 또 가네요.\n",
              "1      위로해 드립니다.\n",
              "2    여행은 언제나 좋죠.\n",
              "3    여행은 언제나 좋죠.\n",
              "4     눈살이 찌푸려지죠.\n",
              "Name: A, dtype: object"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# text와 pair가 쌍으로 이루어짐\n",
        "print(\"Q:\", corpus['Q'][0])  #하나의 문장을 확인하고 싶을 땐 'x[index]' 활용\n",
        "print(\"A:\", corpus['A'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaqifLzKLDee",
        "outputId": "b9f5c44b-7386-4170-cc0d-080d6975067b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: 12시 땡!\n",
            "A: 하루가 또 가네요.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터 전처리"
      ],
      "metadata": {
        "id": "ULbjaPi8LHqY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 전체 코퍼스 크기 (11,823개)\n",
        "corpus.shape[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8qSOcEWLF6B",
        "outputId": "6881283a-8a9f-4f0a-ebe4-fe9bdae57ec3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11823"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2,000개 데이터 셋만 활용 (Google Colab 일 경우 3,000개에서는 메모리 오버되는 현상 발생)\n",
        "texts = []\n",
        "pairs = []\n",
        "for i, (text, pair) in enumerate(zip(corpus['Q'], corpus['A'])):\n",
        "    texts.append(text)\n",
        "    pairs.append(pair)\n",
        "    if i >= 2000:\n",
        "        break"
      ],
      "metadata": {
        "id": "8fserXm2LZ3V"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# question와 answer 데이터 확인\n",
        "list(zip(texts, pairs))[1995:2000]  #끝부분 샘플 5개"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2-H-OGvLeIA",
        "outputId": "02480620-a0ec-4867-a4a4-0f9c971d4245"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('배 아프다', '약이 필요하면 도움을 받아보세요.'),\n",
              " ('배 터지겠네', '위를 좀 쉬게 해주세요.'),\n",
              " ('배 터지겠다.', '산책 좀 해야겠네여.'),\n",
              " ('배가 너무 고파', '뭐 좀 챙겨드세요.'),\n",
              " ('배가 넘넘 고파', '저도 밥 먹고 싶어요')]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 전처리 함수 (정규식(regex expression)활용)\n",
        "import re  #파이썬에서는 re 모듈을 통해 정규표현식을 사용\n",
        "def clean_sentence(sentence):\n",
        "    # 한글, 숫자를 제외한 모든 문자는 제거합니다.\n",
        "    sentence = re.sub(r'[^0-9ㄱ-ㅎㅏ-ㅣ가-힣 ]',r'', sentence)\n",
        "    return sentence"
      ],
      "metadata": {
        "id": "moLQ5ckELhji"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 전처리 함수 테스트\n",
        "print(clean_sentence('안녕하세요~:)'))\n",
        "print(clean_sentence('텐서플로^@^%#@!'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jeME6PbVLnoM",
        "outputId": "23696587-4516-485f-c20b-646606d0392d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "안녕하세요\n",
            "텐서플로\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "한글 형태소 분석기 (Konlpy)"
      ],
      "metadata": {
        "id": "-ZowvkkGLrju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# konlpy 설치\n",
        "!pip install konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ruKP43iWLp66",
        "outputId": "8a04d10e-52f9-4ca8-a6e2-43be9eda0f50"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading JPype1-1.4.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.3/465.3 kB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.3)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (23.1)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.4.1 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Okt형태소\n",
        "from konlpy.tag import Okt\n",
        "okt = Okt()\n",
        "def process_morph(sentence):\n",
        "    return ' '.join(okt.morphs(sentence))  #매개변수를 합침"
      ],
      "metadata": {
        "id": "FZ8-dzrJLupT"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 문장 전처리 함수 정의\n",
        "def clean_and_morph(sentence, is_question=True):\n",
        "    # 한글 문장 전처리\n",
        "    sentence = clean_sentence(sentence)\n",
        "    # 형태소 변환\n",
        "    sentence = process_morph(sentence)\n",
        "    # Question 인 경우, Answer인 경우를 분기하여 처리합니다.\n",
        "    if is_question:\n",
        "        return sentence\n",
        "    else:\n",
        "        # START 토큰은 decoder input에 END 토큰은 decoder output에 추가합니다.\n",
        "        return ('<START> ' + sentence, sentence + ' <END>')\n",
        "\n",
        "def preprocess(texts, pairs):\n",
        "    questions = []\n",
        "    answer_in = []\n",
        "    answer_out = []\n",
        "\n",
        "    # 질의에 대한 전처리\n",
        "    for text in texts:\n",
        "        # 전처리와 morph 수행\n",
        "        question = clean_and_morph(text, is_question=True)\n",
        "        questions.append(question)\n",
        "\n",
        "    # 답변에 대한 전처리\n",
        "    for pair in pairs:\n",
        "        # 전처리와 morph 수행\n",
        "        in_, out_ = clean_and_morph(pair, is_question=False)\n",
        "        answer_in.append(in_)\n",
        "        answer_out.append(out_)\n",
        "\n",
        "    return questions, answer_in, answer_out\n",
        "\n",
        "# 샘플 출력\n",
        "questions, answer_in, answer_out = preprocess(texts, pairs)\n",
        "print(questions[:2])\n",
        "print(answer_in[:2])\n",
        "print(answer_out[:2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pw-V23G0L0HM",
        "outputId": "330de55f-5078-4975-b8fb-5042a6f11033"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['12시 땡', '1 지망 학교 떨어졌어']\n",
            "['<START> 하루 가 또 가네요', '<START> 위로 해 드립니다']\n",
            "['하루 가 또 가네요 <END>', '위로 해 드립니다 <END>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 전체 문장을 하나의 리스트로 만들기\n",
        "all_sentences = questions + answer_in + answer_out"
      ],
      "metadata": {
        "id": "y-MBNEaFL7gi"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "토큰화"
      ],
      "metadata": {
        "id": "ec0gJUNeL-tK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 라이브러리 불어오기\n",
        "import numpy as np\n",
        "import warnings\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# WARNING 무시\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "HkeVTdqgL9Y-"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(filters='', lower=False, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(all_sentences)"
      ],
      "metadata": {
        "id": "eNg3rB9rMBs1"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어사전 확인\n",
        "for word, idx in tokenizer.word_index.items():\n",
        "    print(f'{word}\\t -> \\t{idx}')\n",
        "    if idx > 10:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogA4mBK6MDae",
        "outputId": "8050638e-2dc2-4de8-c46d-a36fab9a13b1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<OOV>\t -> \t1\n",
            "<START>\t -> \t2\n",
            "<END>\t -> \t3\n",
            "이\t -> \t4\n",
            "을\t -> \t5\n",
            "거\t -> \t6\n",
            "가\t -> \t7\n",
            "예요\t -> \t8\n",
            "도\t -> \t9\n",
            "해보세요\t -> \t10\n",
            "요\t -> \t11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 토큰 갯수 확인\n",
        "len(tokenizer.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPOffjK8MFZA",
        "outputId": "6788a282-7595-4472-faee-0586a1395995"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3604"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "치환: 텍스트를 시퀀스로 인코딩 (texts_to_sequences)"
      ],
      "metadata": {
        "id": "Yvq7rq35MIpV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 치환: 텍스트를 시퀀스로 인코딩 (texts_to_sequences)\n",
        "question_sequence = tokenizer.texts_to_sequences(questions)\n",
        "answer_in_sequence = tokenizer.texts_to_sequences(answer_in)\n",
        "answer_out_sequence = tokenizer.texts_to_sequences(answer_out)\n",
        "\n",
        "# 문장의 길이 맞추기 (pad_sequences)\n",
        "MAX_LENGTH = 30\n",
        "question_padded = pad_sequences(question_sequence,\n",
        "                                maxlen=MAX_LENGTH,\n",
        "                                truncating='post',  #뒷부분 자름\n",
        "                                padding='post')  #뒤에 0 채움\n",
        "answer_in_padded = pad_sequences(answer_in_sequence,\n",
        "                                 maxlen=MAX_LENGTH,\n",
        "                                 truncating='post',\n",
        "                                 padding='post')\n",
        "answer_out_padded = pad_sequences(answer_out_sequence,\n",
        "                                  maxlen=MAX_LENGTH,\n",
        "                                  truncating='post',\n",
        "                                  padding='post')"
      ],
      "metadata": {
        "id": "E1syZ0d3MHRP"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델"
      ],
      "metadata": {
        "id": "OrEc0a5fMTOw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 라이브러리 로드\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint"
      ],
      "metadata": {
        "id": "jjBEx31iMOLa"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습용 인코더 (Encoder)"
      ],
      "metadata": {
        "id": "AWiscR5xMX6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 인코더\n",
        "class Encoder(tf.keras.Model):  #tf.keras.Model 상속받음\n",
        "    def __init__(self, units, vocab_size, embedding_dim, time_steps):  #4개의 파라미터\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = Embedding(vocab_size,\n",
        "                                   embedding_dim,\n",
        "                                   input_length=time_steps)\n",
        "        self.dropout = Dropout(0.2)\n",
        "        self.lstm = LSTM(units, return_state=True)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.embedding(inputs)\n",
        "        x = self.dropout(x)\n",
        "        x, hidden_state, cell_state = self.lstm(x)\n",
        "        return [hidden_state, cell_state]"
      ],
      "metadata": {
        "id": "iXqCLGOhMWKI"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습용 디코더 (Decoder)"
      ],
      "metadata": {
        "id": "KExQ-wDTMiTH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 디코더\n",
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, units, vocab_size, embedding_dim, time_steps):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = Embedding(vocab_size,\n",
        "                                   embedding_dim,\n",
        "                                   input_length=time_steps)\n",
        "        self.dropout = Dropout(0.2)\n",
        "        self.lstm = LSTM(units,\n",
        "                         return_state=True,\n",
        "                         return_sequences=True,\n",
        "                        )\n",
        "        self.dense = Dense(vocab_size, activation='softmax')\n",
        "\n",
        "    def call(self, inputs, initial_state):\n",
        "        x = self.embedding(inputs)\n",
        "        x = self.dropout(x)\n",
        "        x, hidden_state, cell_state = self.lstm(x, initial_state=initial_state)\n",
        "        x = self.dense(x)\n",
        "        return x, hidden_state, cell_state"
      ],
      "metadata": {
        "id": "4b2gYgLwMewE"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델 결합"
      ],
      "metadata": {
        "id": "hRWAaKQPMmA4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 결합\n",
        "class Seq2Seq(tf.keras.Model):\n",
        "    def __init__(self, units, vocab_size, embedding_dim, time_steps, start_token, end_token):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.start_token = start_token\n",
        "        self.end_token = end_token\n",
        "        self.time_steps = time_steps\n",
        "\n",
        "        self.encoder = Encoder(units, vocab_size, embedding_dim, time_steps)\n",
        "        self.decoder = Decoder(units, vocab_size, embedding_dim, time_steps)\n",
        "\n",
        "    def call(self, inputs, training=True):\n",
        "        if training:\n",
        "            encoder_inputs, decoder_inputs = inputs\n",
        "            context_vector = self.encoder(encoder_inputs)  #인코더의 출력 결과를 context vector로 저장\n",
        "            decoder_outputs, _, _ = self.decoder(inputs=decoder_inputs,\n",
        "                                                 initial_state=context_vector)  #은닉상태와 셀상태는 훈련에 필요 없음\n",
        "            return decoder_outputs\n",
        "        else:\n",
        "            context_vector = self.encoder(inputs)  #인코더의 출력 결과를 context vector로 저장\n",
        "            target_seq = tf.constant([[self.start_token]], dtype=tf.float32)\n",
        "            results = tf.TensorArray(tf.int32, self.time_steps)\n",
        "\n",
        "            for i in tf.range(self.time_steps):\n",
        "                decoder_output, decoder_hidden, decoder_cell = self.decoder(target_seq,\n",
        "                                                                            initial_state=context_vector)  #디코더에 입력문장과 맥락벡터를 함께 입력값으로 넣음\n",
        "                decoder_output = tf.cast(tf.argmax(decoder_output, axis=-1),\n",
        "                                         dtype=tf.int32)\n",
        "                decoder_output = tf.reshape(decoder_output, shape=(1, 1))\n",
        "                results = results.write(i, decoder_output)\n",
        "\n",
        "                if decoder_output == self.end_token:\n",
        "                    break\n",
        "\n",
        "                target_seq = decoder_output\n",
        "                context_vector = [decoder_hidden, decoder_cell]\n",
        "\n",
        "            return tf.reshape(results.stack(), shape=(1, self.time_steps))"
      ],
      "metadata": {
        "id": "piQYQBTuMk1q"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "단어별 원핫인코딩 적용"
      ],
      "metadata": {
        "id": "H6ryU6GLNANh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE = len(tokenizer.word_index)+1"
      ],
      "metadata": {
        "id": "jWsVuaSsMoMe"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_one_hot(padded):\n",
        "    # 원핫인코딩 초기화\n",
        "    one_hot_vector = np.zeros((len(answer_out_padded),\n",
        "                               MAX_LENGTH,\n",
        "                               VOCAB_SIZE))\n",
        "\n",
        "    # 디코더 목표(출력)를 원핫인코딩으로 변환\n",
        "    # 학습시 입력은 인덱스이지만, 출력은 원핫인코딩 형식임\n",
        "    for i, sequence in enumerate(answer_out_padded):\n",
        "        for j, index in enumerate(sequence):\n",
        "            one_hot_vector[i, j, index] = 1\n",
        "\n",
        "    return one_hot_vector\n",
        "\n",
        "answer_in_one_hot = convert_to_one_hot(answer_in_padded)\n",
        "answer_out_one_hot = convert_to_one_hot(answer_out_padded)"
      ],
      "metadata": {
        "id": "riJ14ygdNCmk"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 변환된 index를 다시 단어로 변환\n",
        "def convert_index_to_text(indexs, end_token):\n",
        "\n",
        "    sentence = ''\n",
        "\n",
        "    # 모든 문장에 대해서 반복\n",
        "    for index in indexs:\n",
        "        if index == end_token:\n",
        "            # 끝 단어이므로 예측 중비\n",
        "            break;\n",
        "        # 사전에 존재하는 단어의 경우 단어 추가\n",
        "        if index > 0 and tokenizer.index_word[index] is not None:\n",
        "            sentence += tokenizer.index_word[index]\n",
        "        else:\n",
        "        # 사전에 없는 인덱스면 빈 문자열 추가\n",
        "            sentence += ''\n",
        "\n",
        "        # 빈칸 추가\n",
        "        sentence += ' '\n",
        "    return sentence"
      ],
      "metadata": {
        "id": "NJU2KrG6NGqU"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습 (Training)"
      ],
      "metadata": {
        "id": "eFaEezRdNLSH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 하이퍼 파라미터 정의\n",
        "BUFFER_SIZE = 1000\n",
        "BATCH_SIZE = 16\n",
        "EMBEDDING_DIM = 100\n",
        "TIME_STEPS = MAX_LENGTH\n",
        "START_TOKEN = tokenizer.word_index['<START>']\n",
        "END_TOKEN = tokenizer.word_index['<END>']\n",
        "\n",
        "UNITS = 128\n",
        "\n",
        "VOCAB_SIZE = len(tokenizer.word_index)+1\n",
        "DATA_LENGTH = len(questions)\n",
        "SAMPLE_SIZE = 3\n",
        "NUM_EPOCHS = 20"
      ],
      "metadata": {
        "id": "r0LQT1jGNJ1n"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 체크포인트 생성\n",
        "checkpoint_path = 'model/seq2seq-chatbot-no-attention-checkpoint.ckpt'\n",
        "checkpoint = ModelCheckpoint(filepath=checkpoint_path,\n",
        "                             save_weights_only=True,\n",
        "                             save_best_only=True,\n",
        "                             monitor='loss',\n",
        "                             verbose=1\n",
        "                            )"
      ],
      "metadata": {
        "id": "YMGTbCmGNN6g"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# seq2seq\n",
        "seq2seq = Seq2Seq(UNITS,\n",
        "                  VOCAB_SIZE,\n",
        "                  EMBEDDING_DIM,\n",
        "                  TIME_STEPS,\n",
        "                  START_TOKEN,\n",
        "                  END_TOKEN)\n",
        "\n",
        "seq2seq.compile(optimizer='adam',\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['acc'])"
      ],
      "metadata": {
        "id": "WxmGwb1kNQX0"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#문장을 예측하는 함수 정의\n",
        "def make_prediction(model, question_inputs):\n",
        "    results = model(inputs=question_inputs, training=False)  #output을 텐서플로 객체 그대로 출력\n",
        "    # 변환된 인덱스를 문장으로 변환\n",
        "    results = np.asarray(results).reshape(-1)  #어떤 값이 되어도 관계x\n",
        "    return results"
      ],
      "metadata": {
        "id": "15UjCY4NNSTL"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f'processing epoch: {epoch * 10 + 1}...')\n",
        "    seq2seq.fit([question_padded, answer_in_padded],\n",
        "                answer_out_one_hot,\n",
        "                epochs=10,\n",
        "                batch_size=BATCH_SIZE,\n",
        "                callbacks=[checkpoint]\n",
        "               )\n",
        "    # 랜덤한 샘플 번호 추출\n",
        "    samples = np.random.randint(DATA_LENGTH, size=SAMPLE_SIZE)\n",
        "\n",
        "    # 예측 성능 테스트\n",
        "    for idx in samples:\n",
        "        question_inputs = question_padded[idx]\n",
        "        # 문장 예측\n",
        "        results = make_prediction(seq2seq, np.expand_dims(question_inputs, 0))\n",
        "\n",
        "        # 변환된 인덱스를 문장으로 변환\n",
        "        results = convert_index_to_text(results, END_TOKEN)\n",
        "\n",
        "        print(f'Q: {questions[idx]}')\n",
        "        print(f'A: {results}\\n')\n",
        "        print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2A-XW-WNcLR",
        "outputId": "fd8ff162-e072-455f-cd2c-e6b323e72c1a"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processing epoch: 1...\n",
            "Epoch 1/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 2.4487 - acc: 0.7929\n",
            "Epoch 1: loss improved from inf to 2.44872, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 32s 139ms/step - loss: 2.4487 - acc: 0.7929\n",
            "Epoch 2/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 1.1961 - acc: 0.8195\n",
            "Epoch 2: loss improved from 2.44872 to 1.19608, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 4s 30ms/step - loss: 1.1961 - acc: 0.8195\n",
            "Epoch 3/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 1.1064 - acc: 0.8371\n",
            "Epoch 3: loss improved from 1.19608 to 1.10638, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 18ms/step - loss: 1.1064 - acc: 0.8371\n",
            "Epoch 4/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 1.0582 - acc: 0.8396\n",
            "Epoch 4: loss improved from 1.10638 to 1.05816, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 4s 28ms/step - loss: 1.0582 - acc: 0.8396\n",
            "Epoch 5/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 1.0266 - acc: 0.8405\n",
            "Epoch 5: loss improved from 1.05816 to 1.02531, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 19ms/step - loss: 1.0253 - acc: 0.8408\n",
            "Epoch 6/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.9994 - acc: 0.8432\n",
            "Epoch 6: loss improved from 1.02531 to 0.99944, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 4s 29ms/step - loss: 0.9994 - acc: 0.8432\n",
            "Epoch 7/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.9759 - acc: 0.8449\n",
            "Epoch 7: loss improved from 0.99944 to 0.97627, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 5s 40ms/step - loss: 0.9763 - acc: 0.8448\n",
            "Epoch 8/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.9537 - acc: 0.8464\n",
            "Epoch 8: loss improved from 0.97627 to 0.95370, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 4s 32ms/step - loss: 0.9537 - acc: 0.8464\n",
            "Epoch 9/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.9335 - acc: 0.8473\n",
            "Epoch 9: loss improved from 0.95370 to 0.93355, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 20ms/step - loss: 0.9335 - acc: 0.8473\n",
            "Epoch 10/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.9164 - acc: 0.8486\n",
            "Epoch 10: loss improved from 0.93355 to 0.91638, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.9164 - acc: 0.8486\n",
            "Q: 단수 래\n",
            "A: 저 을 더 해보세요 \n",
            "\n",
            "\n",
            "Q: 누구 나 힘든거지\n",
            "A: 저 도 더 해보세요 \n",
            "\n",
            "\n",
            "Q: 그냥 이렇게 살 고 싶어\n",
            "A: 저 을 있을 보세요 \n",
            "\n",
            "\n",
            "processing epoch: 11...\n",
            "Epoch 1/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.9012 - acc: 0.8492\n",
            "Epoch 1: loss improved from 0.91638 to 0.90122, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 5s 38ms/step - loss: 0.9012 - acc: 0.8492\n",
            "Epoch 2/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.8855 - acc: 0.8506\n",
            "Epoch 2: loss improved from 0.90122 to 0.88636, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 3s 22ms/step - loss: 0.8864 - acc: 0.8505\n",
            "Epoch 3/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.8712 - acc: 0.8520\n",
            "Epoch 3: loss improved from 0.88636 to 0.87123, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 17ms/step - loss: 0.8712 - acc: 0.8520\n",
            "Epoch 4/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.8559 - acc: 0.8544\n",
            "Epoch 4: loss improved from 0.87123 to 0.85589, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 16ms/step - loss: 0.8559 - acc: 0.8544\n",
            "Epoch 5/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.8421 - acc: 0.8557\n",
            "Epoch 5: loss improved from 0.85589 to 0.84231, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 16ms/step - loss: 0.8423 - acc: 0.8557\n",
            "Epoch 6/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.8234 - acc: 0.8592\n",
            "Epoch 6: loss improved from 0.84231 to 0.82341, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 19ms/step - loss: 0.8234 - acc: 0.8592\n",
            "Epoch 7/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.8060 - acc: 0.8601\n",
            "Epoch 7: loss improved from 0.82341 to 0.80600, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.8060 - acc: 0.8601\n",
            "Epoch 8/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.7863 - acc: 0.8633\n",
            "Epoch 8: loss improved from 0.80600 to 0.78657, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.7866 - acc: 0.8633\n",
            "Epoch 9/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.7690 - acc: 0.8653\n",
            "Epoch 9: loss improved from 0.78657 to 0.76759, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.7676 - acc: 0.8656\n",
            "Epoch 10/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.7493 - acc: 0.8676\n",
            "Epoch 10: loss improved from 0.76759 to 0.74927, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 16ms/step - loss: 0.7493 - acc: 0.8676\n",
            "Q: 나 아 재인 가\n",
            "A: 저 도 요 \n",
            "\n",
            "\n",
            "Q: 내 여자친구 아이돌 이야\n",
            "A: 저 도 요 \n",
            "\n",
            "\n",
            "Q: 미세먼지 최악 인데\n",
            "A: 저 도 될 거 예요 \n",
            "\n",
            "\n",
            "processing epoch: 21...\n",
            "Epoch 1/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.7276 - acc: 0.8704\n",
            "Epoch 1: loss improved from 0.74927 to 0.73021, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 20ms/step - loss: 0.7302 - acc: 0.8700\n",
            "Epoch 2/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.7125 - acc: 0.8720\n",
            "Epoch 2: loss improved from 0.73021 to 0.71247, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.7125 - acc: 0.8720\n",
            "Epoch 3/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.6954 - acc: 0.8743\n",
            "Epoch 3: loss improved from 0.71247 to 0.69541, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 17ms/step - loss: 0.6954 - acc: 0.8743\n",
            "Epoch 4/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.6770 - acc: 0.8765\n",
            "Epoch 4: loss improved from 0.69541 to 0.67713, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 19ms/step - loss: 0.6771 - acc: 0.8765\n",
            "Epoch 5/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.6584 - acc: 0.8795\n",
            "Epoch 5: loss improved from 0.67713 to 0.66018, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 17ms/step - loss: 0.6602 - acc: 0.8791\n",
            "Epoch 6/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.6464 - acc: 0.8811\n",
            "Epoch 6: loss improved from 0.66018 to 0.64620, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 18ms/step - loss: 0.6462 - acc: 0.8811\n",
            "Epoch 7/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.6271 - acc: 0.8835\n",
            "Epoch 7: loss improved from 0.64620 to 0.62706, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 19ms/step - loss: 0.6271 - acc: 0.8835\n",
            "Epoch 8/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.6094 - acc: 0.8866\n",
            "Epoch 8: loss improved from 0.62706 to 0.60936, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 18ms/step - loss: 0.6094 - acc: 0.8866\n",
            "Epoch 9/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.5928 - acc: 0.8890\n",
            "Epoch 9: loss improved from 0.60936 to 0.59265, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 18ms/step - loss: 0.5927 - acc: 0.8891\n",
            "Epoch 10/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.5767 - acc: 0.8916\n",
            "Epoch 10: loss improved from 0.59265 to 0.57676, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 17ms/step - loss: 0.5768 - acc: 0.8918\n",
            "Q: 남편 이 회식 하면 늦게 들어와\n",
            "A: 저 도 듣고 싶네요 \n",
            "\n",
            "\n",
            "Q: 교양 수업 시간 에 마음 에 드는 애 있어\n",
            "A: 저 도 듣고 싶네요 \n",
            "\n",
            "\n",
            "Q: 냄새 날 것 같아 걱정 이야\n",
            "A: 저 도 궁금하네요 \n",
            "\n",
            "\n",
            "processing epoch: 31...\n",
            "Epoch 1/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.5607 - acc: 0.8943\n",
            "Epoch 1: loss improved from 0.57676 to 0.56070, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 3s 24ms/step - loss: 0.5607 - acc: 0.8943\n",
            "Epoch 2/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.5452 - acc: 0.8977\n",
            "Epoch 2: loss improved from 0.56070 to 0.54495, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.5449 - acc: 0.8977\n",
            "Epoch 3/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.5301 - acc: 0.9007\n",
            "Epoch 3: loss improved from 0.54495 to 0.53013, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 19ms/step - loss: 0.5301 - acc: 0.9007\n",
            "Epoch 4/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.5152 - acc: 0.9024\n",
            "Epoch 4: loss improved from 0.53013 to 0.51507, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 3s 26ms/step - loss: 0.5151 - acc: 0.9024\n",
            "Epoch 5/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.5009 - acc: 0.9063\n",
            "Epoch 5: loss improved from 0.51507 to 0.50092, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.5009 - acc: 0.9063\n",
            "Epoch 6/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.4869 - acc: 0.9089\n",
            "Epoch 6: loss improved from 0.50092 to 0.48667, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 3s 22ms/step - loss: 0.4867 - acc: 0.9089\n",
            "Epoch 7/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.4736 - acc: 0.9111\n",
            "Epoch 7: loss improved from 0.48667 to 0.47315, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.4732 - acc: 0.9112\n",
            "Epoch 8/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.4612 - acc: 0.9135\n",
            "Epoch 8: loss improved from 0.47315 to 0.46021, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 19ms/step - loss: 0.4602 - acc: 0.9137\n",
            "Epoch 9/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.4472 - acc: 0.9166\n",
            "Epoch 9: loss improved from 0.46021 to 0.44718, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 19ms/step - loss: 0.4472 - acc: 0.9166\n",
            "Epoch 10/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.4368 - acc: 0.9182\n",
            "Epoch 10: loss improved from 0.44718 to 0.43624, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 16ms/step - loss: 0.4362 - acc: 0.9183\n",
            "Q: 마음 이 복잡해\n",
            "A: 저 도 궁금하네요 \n",
            "\n",
            "\n",
            "Q: 낚시 는 무슨 재미\n",
            "A: 잘 하고 있어요 \n",
            "\n",
            "\n",
            "Q: 면도 해야 되는데 귀찮아\n",
            "A: 잘 해보세요 \n",
            "\n",
            "\n",
            "processing epoch: 41...\n",
            "Epoch 1/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.4252 - acc: 0.9208\n",
            "Epoch 1: loss improved from 0.43624 to 0.42487, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.4249 - acc: 0.9210\n",
            "Epoch 2/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.4147 - acc: 0.9235\n",
            "Epoch 2: loss improved from 0.42487 to 0.41392, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.4139 - acc: 0.9237\n",
            "Epoch 3/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.4046 - acc: 0.9252\n",
            "Epoch 3: loss improved from 0.41392 to 0.40432, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.4043 - acc: 0.9253\n",
            "Epoch 4/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.3951 - acc: 0.9268\n",
            "Epoch 4: loss improved from 0.40432 to 0.39505, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 3s 25ms/step - loss: 0.3950 - acc: 0.9269\n",
            "Epoch 5/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.3858 - acc: 0.9292\n",
            "Epoch 5: loss improved from 0.39505 to 0.38598, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 3s 22ms/step - loss: 0.3860 - acc: 0.9291\n",
            "Epoch 6/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.3776 - acc: 0.9300\n",
            "Epoch 6: loss improved from 0.38598 to 0.37756, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 19ms/step - loss: 0.3776 - acc: 0.9300\n",
            "Epoch 7/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.3685 - acc: 0.9321\n",
            "Epoch 7: loss improved from 0.37756 to 0.36880, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.3688 - acc: 0.9320\n",
            "Epoch 8/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.3608 - acc: 0.9336\n",
            "Epoch 8: loss improved from 0.36880 to 0.36115, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.3612 - acc: 0.9334\n",
            "Epoch 9/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.3539 - acc: 0.9352\n",
            "Epoch 9: loss improved from 0.36115 to 0.35385, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 16ms/step - loss: 0.3538 - acc: 0.9352\n",
            "Epoch 10/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.3469 - acc: 0.9357\n",
            "Epoch 10: loss improved from 0.35385 to 0.34601, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 3s 23ms/step - loss: 0.3460 - acc: 0.9359\n",
            "Q: 넘어질 뻔했어\n",
            "A: 제 가 있잖아요 \n",
            "\n",
            "\n",
            "Q: 길이 미끄러워서 미끄러질 뻔했어\n",
            "A: 잘 하고 있어요 \n",
            "\n",
            "\n",
            "Q: 내 가 멍청한거지\n",
            "A: 저 도 궁금하네요 \n",
            "\n",
            "\n",
            "processing epoch: 51...\n",
            "Epoch 1/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.3400 - acc: 0.9374\n",
            "Epoch 1: loss improved from 0.34601 to 0.33998, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 4s 32ms/step - loss: 0.3400 - acc: 0.9374\n",
            "Epoch 2/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.3341 - acc: 0.9387\n",
            "Epoch 2: loss improved from 0.33998 to 0.33412, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 4s 29ms/step - loss: 0.3341 - acc: 0.9387\n",
            "Epoch 3/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.3278 - acc: 0.9399\n",
            "Epoch 3: loss improved from 0.33412 to 0.32780, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 3s 22ms/step - loss: 0.3278 - acc: 0.9398\n",
            "Epoch 4/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.3213 - acc: 0.9406\n",
            "Epoch 4: loss improved from 0.32780 to 0.32134, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 3s 24ms/step - loss: 0.3213 - acc: 0.9406\n",
            "Epoch 5/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.3164 - acc: 0.9412\n",
            "Epoch 5: loss improved from 0.32134 to 0.31642, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 18ms/step - loss: 0.3164 - acc: 0.9412\n",
            "Epoch 6/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.3116 - acc: 0.9423\n",
            "Epoch 6: loss improved from 0.31642 to 0.31159, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 3s 26ms/step - loss: 0.3116 - acc: 0.9423\n",
            "Epoch 7/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.3070 - acc: 0.9432\n",
            "Epoch 7: loss improved from 0.31159 to 0.30671, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.3067 - acc: 0.9433\n",
            "Epoch 8/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.3021 - acc: 0.9441\n",
            "Epoch 8: loss improved from 0.30671 to 0.30208, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.3021 - acc: 0.9441\n",
            "Epoch 9/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.2979 - acc: 0.9452\n",
            "Epoch 9: loss improved from 0.30208 to 0.29798, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.2980 - acc: 0.9452\n",
            "Epoch 10/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.2938 - acc: 0.9454\n",
            "Epoch 10: loss improved from 0.29798 to 0.29343, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 16ms/step - loss: 0.2934 - acc: 0.9455\n",
            "Q: 내 인생 은 가시 밭길 같아\n",
            "A: 저 도 궁금하네요 \n",
            "\n",
            "\n",
            "Q: 가끔 궁금해\n",
            "A: 잘 하고 있어요 \n",
            "\n",
            "\n",
            "Q: 나 만 제자리걸음 이야\n",
            "A: 저 도 듣고 싶어요 \n",
            "\n",
            "\n",
            "processing epoch: 61...\n",
            "Epoch 1/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.2891 - acc: 0.9460\n",
            "Epoch 1: loss improved from 0.29343 to 0.28962, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.2896 - acc: 0.9459\n",
            "Epoch 2/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.2856 - acc: 0.9465\n",
            "Epoch 2: loss improved from 0.28962 to 0.28563, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.2856 - acc: 0.9465\n",
            "Epoch 3/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.2820 - acc: 0.9472\n",
            "Epoch 3: loss improved from 0.28563 to 0.28231, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.2823 - acc: 0.9470\n",
            "Epoch 4/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.2794 - acc: 0.9481\n",
            "Epoch 4: loss improved from 0.28231 to 0.27941, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.2794 - acc: 0.9481\n",
            "Epoch 5/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.2763 - acc: 0.9484\n",
            "Epoch 5: loss improved from 0.27941 to 0.27621, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.2762 - acc: 0.9484\n",
            "Epoch 6/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.2718 - acc: 0.9493\n",
            "Epoch 6: loss improved from 0.27621 to 0.27206, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 3s 23ms/step - loss: 0.2721 - acc: 0.9492\n",
            "Epoch 7/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.2689 - acc: 0.9499\n",
            "Epoch 7: loss improved from 0.27206 to 0.26889, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 17ms/step - loss: 0.2689 - acc: 0.9499\n",
            "Epoch 8/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.2670 - acc: 0.9502\n",
            "Epoch 8: loss improved from 0.26889 to 0.26731, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 17ms/step - loss: 0.2673 - acc: 0.9501\n",
            "Epoch 9/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.2643 - acc: 0.9508\n",
            "Epoch 9: loss improved from 0.26731 to 0.26429, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 16ms/step - loss: 0.2643 - acc: 0.9508\n",
            "Epoch 10/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.2613 - acc: 0.9513\n",
            "Epoch 10: loss improved from 0.26429 to 0.26148, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.2615 - acc: 0.9513\n",
            "Q: 맥주 소주 어떤거 마실까\n",
            "A: 제 가 있잖아요 \n",
            "\n",
            "\n",
            "Q: 내 가 이상한가\n",
            "A: 저 도 보고 싶어요 \n",
            "\n",
            "\n",
            "Q: 꿈 이 다양해\n",
            "A: 저 도 듣고 싶네요 \n",
            "\n",
            "\n",
            "processing epoch: 71...\n",
            "Epoch 1/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.2611 - acc: 0.9511\n",
            "Epoch 1: loss did not improve from 0.26148\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.2617 - acc: 0.9510\n",
            "Epoch 2/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.2585 - acc: 0.9513\n",
            "Epoch 2: loss improved from 0.26148 to 0.25860, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 3s 27ms/step - loss: 0.2586 - acc: 0.9513\n",
            "Epoch 3/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.2551 - acc: 0.9520\n",
            "Epoch 3: loss improved from 0.25860 to 0.25501, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 17ms/step - loss: 0.2550 - acc: 0.9520\n",
            "Epoch 4/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.2529 - acc: 0.9515\n",
            "Epoch 4: loss improved from 0.25501 to 0.25282, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.2528 - acc: 0.9515\n",
            "Epoch 5/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.2500 - acc: 0.9529\n",
            "Epoch 5: loss improved from 0.25282 to 0.25003, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.2500 - acc: 0.9529\n",
            "Epoch 6/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.2477 - acc: 0.9533\n",
            "Epoch 6: loss improved from 0.25003 to 0.24769, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.2477 - acc: 0.9533\n",
            "Epoch 7/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.2464 - acc: 0.9531\n",
            "Epoch 7: loss improved from 0.24769 to 0.24641, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 16ms/step - loss: 0.2464 - acc: 0.9531\n",
            "Epoch 8/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.2445 - acc: 0.9535\n",
            "Epoch 8: loss improved from 0.24641 to 0.24448, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 19ms/step - loss: 0.2445 - acc: 0.9535\n",
            "Epoch 9/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.2431 - acc: 0.9533\n",
            "Epoch 9: loss improved from 0.24448 to 0.24283, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 16ms/step - loss: 0.2428 - acc: 0.9534\n",
            "Epoch 10/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.2410 - acc: 0.9537\n",
            "Epoch 10: loss improved from 0.24283 to 0.24076, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.2408 - acc: 0.9538\n",
            "Q: 결정 은 빠르면 빠를 수록 좋겠지\n",
            "A: 저 도 듣고 싶네요 \n",
            "\n",
            "\n",
            "Q: 골프 치러 가야 돼\n",
            "A: 잘 하고 있어요 당당해지세요 \n",
            "\n",
            "\n",
            "Q: 결혼 이나 하지 왜 자꾸 나 한테 화 내냐구\n",
            "A: 저 도 보고 싶어요 \n",
            "\n",
            "\n",
            "processing epoch: 81...\n",
            "Epoch 1/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.2401 - acc: 0.9545\n",
            "Epoch 1: loss improved from 0.24076 to 0.24009, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.2401 - acc: 0.9545\n",
            "Epoch 2/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.2379 - acc: 0.9548\n",
            "Epoch 2: loss improved from 0.24009 to 0.23786, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.2379 - acc: 0.9547\n",
            "Epoch 3/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.2365 - acc: 0.9544\n",
            "Epoch 3: loss improved from 0.23786 to 0.23653, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 17ms/step - loss: 0.2365 - acc: 0.9544\n",
            "Epoch 4/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.2346 - acc: 0.9552\n",
            "Epoch 4: loss improved from 0.23653 to 0.23463, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 19ms/step - loss: 0.2346 - acc: 0.9552\n",
            "Epoch 5/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.2322 - acc: 0.9554\n",
            "Epoch 5: loss improved from 0.23463 to 0.23219, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.2322 - acc: 0.9554\n",
            "Epoch 6/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.2315 - acc: 0.9557\n",
            "Epoch 6: loss improved from 0.23219 to 0.23176, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.2318 - acc: 0.9557\n",
            "Epoch 7/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.2293 - acc: 0.9560\n",
            "Epoch 7: loss improved from 0.23176 to 0.22931, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.2293 - acc: 0.9560\n",
            "Epoch 8/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.2281 - acc: 0.9558\n",
            "Epoch 8: loss improved from 0.22931 to 0.22808, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.2281 - acc: 0.9559\n",
            "Epoch 9/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.2272 - acc: 0.9564\n",
            "Epoch 9: loss improved from 0.22808 to 0.22714, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 17ms/step - loss: 0.2271 - acc: 0.9564\n",
            "Epoch 10/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.2260 - acc: 0.9566\n",
            "Epoch 10: loss improved from 0.22714 to 0.22595, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.2260 - acc: 0.9566\n",
            "Q: 늦은 시간 에 걸어다니니까 무서워\n",
            "A: 저 도 궁금하네요 \n",
            "\n",
            "\n",
            "Q: 나 폰겜 너무 많이 해\n",
            "A: 잘 해보세요 \n",
            "\n",
            "\n",
            "Q: 누구 냐 넌\n",
            "A: 저 는 위로 해드리는 로봇 이에요 \n",
            "\n",
            "\n",
            "processing epoch: 91...\n",
            "Epoch 1/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.2241 - acc: 0.9568\n",
            "Epoch 1: loss improved from 0.22595 to 0.22406, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 18ms/step - loss: 0.2241 - acc: 0.9568\n",
            "Epoch 2/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.2231 - acc: 0.9567\n",
            "Epoch 2: loss improved from 0.22406 to 0.22309, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 17ms/step - loss: 0.2231 - acc: 0.9567\n",
            "Epoch 3/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.2226 - acc: 0.9570\n",
            "Epoch 3: loss improved from 0.22309 to 0.22259, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.2226 - acc: 0.9570\n",
            "Epoch 4/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.2211 - acc: 0.9573\n",
            "Epoch 4: loss improved from 0.22259 to 0.22112, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.2211 - acc: 0.9573\n",
            "Epoch 5/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.2201 - acc: 0.9573\n",
            "Epoch 5: loss improved from 0.22112 to 0.22009, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.2201 - acc: 0.9573\n",
            "Epoch 6/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.2190 - acc: 0.9576\n",
            "Epoch 6: loss improved from 0.22009 to 0.21903, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 17ms/step - loss: 0.2190 - acc: 0.9576\n",
            "Epoch 7/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.2174 - acc: 0.9580\n",
            "Epoch 7: loss improved from 0.21903 to 0.21722, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 3s 22ms/step - loss: 0.2172 - acc: 0.9581\n",
            "Epoch 8/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.2164 - acc: 0.9580\n",
            "Epoch 8: loss improved from 0.21722 to 0.21645, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.2164 - acc: 0.9580\n",
            "Epoch 9/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.2156 - acc: 0.9577\n",
            "Epoch 9: loss improved from 0.21645 to 0.21555, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.2156 - acc: 0.9577\n",
            "Epoch 10/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.2140 - acc: 0.9585\n",
            "Epoch 10: loss improved from 0.21555 to 0.21413, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 16ms/step - loss: 0.2141 - acc: 0.9584\n",
            "Q: 다음 에 봐\n",
            "A: 저 도 보고 싶어요 \n",
            "\n",
            "\n",
            "Q: 내 가 이상한가\n",
            "A: 저 도 보고 싶어요 \n",
            "\n",
            "\n",
            "Q: 내 가 너무 방심했어\n",
            "A: 저 도 보고 싶어요 \n",
            "\n",
            "\n",
            "processing epoch: 101...\n",
            "Epoch 1/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.2144 - acc: 0.9584\n",
            "Epoch 1: loss did not improve from 0.21413\n",
            "126/126 [==============================] - 2s 18ms/step - loss: 0.2144 - acc: 0.9584\n",
            "Epoch 2/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.2130 - acc: 0.9582\n",
            "Epoch 2: loss improved from 0.21413 to 0.21296, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 19ms/step - loss: 0.2130 - acc: 0.9582\n",
            "Epoch 3/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.2127 - acc: 0.9581\n",
            "Epoch 3: loss improved from 0.21296 to 0.21271, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 19ms/step - loss: 0.2127 - acc: 0.9581\n",
            "Epoch 4/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.2109 - acc: 0.9583\n",
            "Epoch 4: loss improved from 0.21271 to 0.21092, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.2109 - acc: 0.9583\n",
            "Epoch 5/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.2108 - acc: 0.9586\n",
            "Epoch 5: loss improved from 0.21092 to 0.21076, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.2108 - acc: 0.9586\n",
            "Epoch 6/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.2092 - acc: 0.9585\n",
            "Epoch 6: loss improved from 0.21076 to 0.20931, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.2093 - acc: 0.9585\n",
            "Epoch 7/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.2093 - acc: 0.9586\n",
            "Epoch 7: loss improved from 0.20931 to 0.20919, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.2092 - acc: 0.9586\n",
            "Epoch 8/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.2079 - acc: 0.9589\n",
            "Epoch 8: loss improved from 0.20919 to 0.20786, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.2079 - acc: 0.9590\n",
            "Epoch 9/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.2077 - acc: 0.9587\n",
            "Epoch 9: loss improved from 0.20786 to 0.20770, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 3s 22ms/step - loss: 0.2077 - acc: 0.9587\n",
            "Epoch 10/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.2067 - acc: 0.9592\n",
            "Epoch 10: loss improved from 0.20770 to 0.20681, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.2068 - acc: 0.9592\n",
            "Q: 남자친구 가 나 안 믿어줘\n",
            "A: 저 도 궁금하네요 \n",
            "\n",
            "\n",
            "Q: 마카롱 먹고 싶다\n",
            "A: 잘 하고 있을 거 예요 \n",
            "\n",
            "\n",
            "Q: 남친 에 내 사진 없어\n",
            "A: 잘 할 수 있을 거 예요 \n",
            "\n",
            "\n",
            "processing epoch: 111...\n",
            "Epoch 1/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.2051 - acc: 0.9592\n",
            "Epoch 1: loss improved from 0.20681 to 0.20511, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 18ms/step - loss: 0.2051 - acc: 0.9593\n",
            "Epoch 2/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.2049 - acc: 0.9594\n",
            "Epoch 2: loss improved from 0.20511 to 0.20494, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.2049 - acc: 0.9594\n",
            "Epoch 3/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.2039 - acc: 0.9594\n",
            "Epoch 3: loss improved from 0.20494 to 0.20392, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 17ms/step - loss: 0.2039 - acc: 0.9594\n",
            "Epoch 4/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.2025 - acc: 0.9597\n",
            "Epoch 4: loss improved from 0.20392 to 0.20242, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 3s 23ms/step - loss: 0.2024 - acc: 0.9598\n",
            "Epoch 5/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.2041 - acc: 0.9593\n",
            "Epoch 5: loss did not improve from 0.20242\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.2041 - acc: 0.9593\n",
            "Epoch 6/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.2011 - acc: 0.9596\n",
            "Epoch 6: loss improved from 0.20242 to 0.20114, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 19ms/step - loss: 0.2011 - acc: 0.9596\n",
            "Epoch 7/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.2009 - acc: 0.9601\n",
            "Epoch 7: loss improved from 0.20114 to 0.20092, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.2009 - acc: 0.9601\n",
            "Epoch 8/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.1997 - acc: 0.9601\n",
            "Epoch 8: loss improved from 0.20092 to 0.19985, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.1999 - acc: 0.9601\n",
            "Epoch 9/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.2001 - acc: 0.9599\n",
            "Epoch 9: loss did not improve from 0.19985\n",
            "126/126 [==============================] - 2s 18ms/step - loss: 0.2001 - acc: 0.9599\n",
            "Epoch 10/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.1996 - acc: 0.9601\n",
            "Epoch 10: loss improved from 0.19985 to 0.19965, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 20ms/step - loss: 0.1997 - acc: 0.9601\n",
            "Q: 문 여 는 소리 같은게 나서 무서움\n",
            "A: 저 도 보고 싶어요 \n",
            "\n",
            "\n",
            "Q: 나 바 본인 가 봄\n",
            "A: 저 도 궁금하네요 \n",
            "\n",
            "\n",
            "Q: 물리치료 받아야겠다\n",
            "A: 잘 해결 되길 바랄게요 \n",
            "\n",
            "\n",
            "processing epoch: 121...\n",
            "Epoch 1/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.1983 - acc: 0.9600\n",
            "Epoch 1: loss improved from 0.19965 to 0.19831, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 4s 28ms/step - loss: 0.1983 - acc: 0.9600\n",
            "Epoch 2/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.1965 - acc: 0.9606\n",
            "Epoch 2: loss improved from 0.19831 to 0.19652, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 16ms/step - loss: 0.1965 - acc: 0.9606\n",
            "Epoch 3/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.1958 - acc: 0.9604\n",
            "Epoch 3: loss improved from 0.19652 to 0.19581, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.1958 - acc: 0.9604\n",
            "Epoch 4/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.1961 - acc: 0.9602\n",
            "Epoch 4: loss did not improve from 0.19581\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.1960 - acc: 0.9602\n",
            "Epoch 5/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.1950 - acc: 0.9607\n",
            "Epoch 5: loss improved from 0.19581 to 0.19488, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.1949 - acc: 0.9607\n",
            "Epoch 6/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.1939 - acc: 0.9608\n",
            "Epoch 6: loss improved from 0.19488 to 0.19415, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.1941 - acc: 0.9608\n",
            "Epoch 7/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.1938 - acc: 0.9608\n",
            "Epoch 7: loss improved from 0.19415 to 0.19367, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 19ms/step - loss: 0.1937 - acc: 0.9609\n",
            "Epoch 8/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.1937 - acc: 0.9609\n",
            "Epoch 8: loss improved from 0.19367 to 0.19366, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 18ms/step - loss: 0.1937 - acc: 0.9609\n",
            "Epoch 9/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.1923 - acc: 0.9609\n",
            "Epoch 9: loss improved from 0.19366 to 0.19223, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.1922 - acc: 0.9610\n",
            "Epoch 10/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.1919 - acc: 0.9608\n",
            "Epoch 10: loss improved from 0.19223 to 0.19185, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.1919 - acc: 0.9608\n",
            "Q: 다리 꼬 지 마\n",
            "A: 저 도 궁금하네요 \n",
            "\n",
            "\n",
            "Q: 방학 이 있었으면 좋겠어\n",
            "A: 저 는 위로 해드리는 로봇 이에요 \n",
            "\n",
            "\n",
            "Q: 막말 최악 이지\n",
            "A: 저 도 궁금하네요 \n",
            "\n",
            "\n",
            "processing epoch: 131...\n",
            "Epoch 1/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.1909 - acc: 0.9614\n",
            "Epoch 1: loss improved from 0.19185 to 0.19085, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 18ms/step - loss: 0.1908 - acc: 0.9614\n",
            "Epoch 2/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.1902 - acc: 0.9616\n",
            "Epoch 2: loss improved from 0.19085 to 0.19030, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.1903 - acc: 0.9616\n",
            "Epoch 3/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.1903 - acc: 0.9612\n",
            "Epoch 3: loss did not improve from 0.19030\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.1905 - acc: 0.9612\n",
            "Epoch 4/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.1902 - acc: 0.9612\n",
            "Epoch 4: loss improved from 0.19030 to 0.19023, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 19ms/step - loss: 0.1902 - acc: 0.9612\n",
            "Epoch 5/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.1889 - acc: 0.9612\n",
            "Epoch 5: loss improved from 0.19023 to 0.18893, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 18ms/step - loss: 0.1889 - acc: 0.9612\n",
            "Epoch 6/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.1882 - acc: 0.9612\n",
            "Epoch 6: loss improved from 0.18893 to 0.18819, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.1882 - acc: 0.9613\n",
            "Epoch 7/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.1890 - acc: 0.9612\n",
            "Epoch 7: loss did not improve from 0.18819\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.1891 - acc: 0.9612\n",
            "Epoch 8/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.1880 - acc: 0.9613\n",
            "Epoch 8: loss improved from 0.18819 to 0.18802, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 16ms/step - loss: 0.1880 - acc: 0.9613\n",
            "Epoch 9/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.1859 - acc: 0.9617\n",
            "Epoch 9: loss improved from 0.18802 to 0.18586, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.1859 - acc: 0.9617\n",
            "Epoch 10/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.1854 - acc: 0.9620\n",
            "Epoch 10: loss improved from 0.18586 to 0.18537, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 19ms/step - loss: 0.1854 - acc: 0.9620\n",
            "Q: 바람 쐬러 정동진 가\n",
            "A: 저 도 궁금하네요 \n",
            "\n",
            "\n",
            "Q: 기댈 수 있는 사람\n",
            "A: 제 가 있잖아요 \n",
            "\n",
            "\n",
            "Q: 놀러 와라\n",
            "A: 더 웃으면서 대해 보세요 \n",
            "\n",
            "\n",
            "processing epoch: 141...\n",
            "Epoch 1/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.1852 - acc: 0.9617\n",
            "Epoch 1: loss improved from 0.18537 to 0.18518, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 3s 22ms/step - loss: 0.1852 - acc: 0.9617\n",
            "Epoch 2/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.1837 - acc: 0.9621\n",
            "Epoch 2: loss improved from 0.18518 to 0.18368, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 17ms/step - loss: 0.1837 - acc: 0.9621\n",
            "Epoch 3/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.1835 - acc: 0.9619\n",
            "Epoch 3: loss did not improve from 0.18368\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.1837 - acc: 0.9619\n",
            "Epoch 4/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.1827 - acc: 0.9619\n",
            "Epoch 4: loss improved from 0.18368 to 0.18270, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.1827 - acc: 0.9619\n",
            "Epoch 5/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.1805 - acc: 0.9622\n",
            "Epoch 5: loss improved from 0.18270 to 0.18071, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.1807 - acc: 0.9622\n",
            "Epoch 6/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.1798 - acc: 0.9625\n",
            "Epoch 6: loss improved from 0.18071 to 0.17983, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.1798 - acc: 0.9624\n",
            "Epoch 7/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.1795 - acc: 0.9624\n",
            "Epoch 7: loss improved from 0.17983 to 0.17952, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 18ms/step - loss: 0.1795 - acc: 0.9624\n",
            "Epoch 8/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.1782 - acc: 0.9624\n",
            "Epoch 8: loss improved from 0.17952 to 0.17831, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 19ms/step - loss: 0.1783 - acc: 0.9624\n",
            "Epoch 9/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.1773 - acc: 0.9626\n",
            "Epoch 9: loss improved from 0.17831 to 0.17732, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.1773 - acc: 0.9626\n",
            "Epoch 10/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.1768 - acc: 0.9627\n",
            "Epoch 10: loss improved from 0.17732 to 0.17689, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 16ms/step - loss: 0.1769 - acc: 0.9627\n",
            "Q: 반지 호수 모르는데\n",
            "A: 저 도 보고 싶어요 \n",
            "\n",
            "\n",
            "Q: 거짓말 을 나도 모르게 자꾸 해\n",
            "A: 그런 생각 을 들게 하는 사람 상종 하지 마세요 \n",
            "\n",
            "\n",
            "Q: 내 가 바보 지\n",
            "A: 저 도 보고 싶어요 \n",
            "\n",
            "\n",
            "processing epoch: 151...\n",
            "Epoch 1/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.1768 - acc: 0.9628\n",
            "Epoch 1: loss improved from 0.17689 to 0.17686, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 16ms/step - loss: 0.1769 - acc: 0.9628\n",
            "Epoch 2/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.1858 - acc: 0.9617\n",
            "Epoch 2: loss did not improve from 0.17686\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.1857 - acc: 0.9618\n",
            "Epoch 3/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.1748 - acc: 0.9628\n",
            "Epoch 3: loss improved from 0.17686 to 0.17477, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.1748 - acc: 0.9628\n",
            "Epoch 4/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.1745 - acc: 0.9627\n",
            "Epoch 4: loss improved from 0.17477 to 0.17442, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 18ms/step - loss: 0.1744 - acc: 0.9628\n",
            "Epoch 5/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.1726 - acc: 0.9629\n",
            "Epoch 5: loss improved from 0.17442 to 0.17260, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 18ms/step - loss: 0.1726 - acc: 0.9629\n",
            "Epoch 6/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.1702 - acc: 0.9640\n",
            "Epoch 6: loss improved from 0.17260 to 0.17023, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.1702 - acc: 0.9640\n",
            "Epoch 7/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.1695 - acc: 0.9632\n",
            "Epoch 7: loss improved from 0.17023 to 0.16952, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.1695 - acc: 0.9633\n",
            "Epoch 8/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.1686 - acc: 0.9635\n",
            "Epoch 8: loss improved from 0.16952 to 0.16863, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 17ms/step - loss: 0.1686 - acc: 0.9635\n",
            "Epoch 9/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.1676 - acc: 0.9630\n",
            "Epoch 9: loss improved from 0.16863 to 0.16780, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.1678 - acc: 0.9630\n",
            "Epoch 10/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.1652 - acc: 0.9642\n",
            "Epoch 10: loss improved from 0.16780 to 0.16518, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 17ms/step - loss: 0.1652 - acc: 0.9642\n",
            "Q: 결혼 하는데 돈 많이 드 네\n",
            "A: 시원하게 씻고 오세요 \n",
            "\n",
            "\n",
            "Q: 머리 가 띵해\n",
            "A: 저 는 위로 해드리는 로봇 이에요 \n",
            "\n",
            "\n",
            "Q: 나 누구 게\n",
            "A: 저 도 궁금하네요 \n",
            "\n",
            "\n",
            "processing epoch: 161...\n",
            "Epoch 1/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.1647 - acc: 0.9636\n",
            "Epoch 1: loss improved from 0.16518 to 0.16467, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 16ms/step - loss: 0.1647 - acc: 0.9636\n",
            "Epoch 2/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.1642 - acc: 0.9635\n",
            "Epoch 2: loss improved from 0.16467 to 0.16419, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.1642 - acc: 0.9635\n",
            "Epoch 3/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.1633 - acc: 0.9637\n",
            "Epoch 3: loss improved from 0.16419 to 0.16326, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.1633 - acc: 0.9637\n",
            "Epoch 4/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.1613 - acc: 0.9641\n",
            "Epoch 4: loss improved from 0.16326 to 0.16127, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.1613 - acc: 0.9641\n",
            "Epoch 5/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.1598 - acc: 0.9641\n",
            "Epoch 5: loss improved from 0.16127 to 0.15985, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 17ms/step - loss: 0.1598 - acc: 0.9641\n",
            "Epoch 6/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.1593 - acc: 0.9643\n",
            "Epoch 6: loss improved from 0.15985 to 0.15936, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1594 - acc: 0.9643\n",
            "Epoch 7/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.1579 - acc: 0.9643\n",
            "Epoch 7: loss improved from 0.15936 to 0.15788, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 16ms/step - loss: 0.1579 - acc: 0.9643\n",
            "Epoch 8/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.1567 - acc: 0.9644\n",
            "Epoch 8: loss improved from 0.15788 to 0.15696, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 16ms/step - loss: 0.1570 - acc: 0.9644\n",
            "Epoch 9/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.1570 - acc: 0.9643\n",
            "Epoch 9: loss improved from 0.15696 to 0.15683, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 17ms/step - loss: 0.1568 - acc: 0.9643\n",
            "Epoch 10/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.1541 - acc: 0.9651\n",
            "Epoch 10: loss improved from 0.15683 to 0.15407, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.1541 - acc: 0.9650\n",
            "Q: 나이 들면서 눈물 이 많아졌어\n",
            "A: 자신 의 감정 을 주변 사람 들 에게 터 놓고 이야기 해보세요 \n",
            "\n",
            "\n",
            "Q: 내 가 너무 생각 없이 말 했어\n",
            "A: 그 때 가 고비 예요 한번 참아 보세요 \n",
            "\n",
            "\n",
            "Q: 더 나은 학교생활 하고 싶어\n",
            "A: 아무 도 없는 곳 으로 여행 을 떠나 보세요 \n",
            "\n",
            "\n",
            "processing epoch: 171...\n",
            "Epoch 1/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.1536 - acc: 0.9649\n",
            "Epoch 1: loss improved from 0.15407 to 0.15354, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 18ms/step - loss: 0.1535 - acc: 0.9649\n",
            "Epoch 2/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.1522 - acc: 0.9651\n",
            "Epoch 2: loss improved from 0.15354 to 0.15236, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1524 - acc: 0.9650\n",
            "Epoch 3/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.1523 - acc: 0.9650\n",
            "Epoch 3: loss improved from 0.15236 to 0.15227, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 17ms/step - loss: 0.1523 - acc: 0.9650\n",
            "Epoch 4/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.1512 - acc: 0.9651\n",
            "Epoch 4: loss improved from 0.15227 to 0.15122, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.1512 - acc: 0.9651\n",
            "Epoch 5/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.1498 - acc: 0.9653\n",
            "Epoch 5: loss improved from 0.15122 to 0.14982, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.1498 - acc: 0.9653\n",
            "Epoch 6/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.1487 - acc: 0.9656\n",
            "Epoch 6: loss improved from 0.14982 to 0.14890, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 16ms/step - loss: 0.1489 - acc: 0.9656\n",
            "Epoch 7/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.1479 - acc: 0.9657\n",
            "Epoch 7: loss improved from 0.14890 to 0.14794, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 17ms/step - loss: 0.1479 - acc: 0.9657\n",
            "Epoch 8/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.1481 - acc: 0.9654\n",
            "Epoch 8: loss did not improve from 0.14794\n",
            "126/126 [==============================] - 2s 19ms/step - loss: 0.1481 - acc: 0.9654\n",
            "Epoch 9/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.1451 - acc: 0.9662\n",
            "Epoch 9: loss improved from 0.14794 to 0.14509, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 18ms/step - loss: 0.1451 - acc: 0.9662\n",
            "Epoch 10/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.1439 - acc: 0.9660\n",
            "Epoch 10: loss improved from 0.14509 to 0.14387, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.1439 - acc: 0.9661\n",
            "Q: 방학 안 끝났으면 좋겠다\n",
            "A: 지금 많이 위축 된 상태 인 것 같습니다 \n",
            "\n",
            "\n",
            "Q: 또 살찐 거 같아\n",
            "A: 자신 의 잠재력 을 믿어 보세요 \n",
            "\n",
            "\n",
            "Q: 내일 친구 랑 놀까\n",
            "A: 잘 골라 보세요 \n",
            "\n",
            "\n",
            "processing epoch: 181...\n",
            "Epoch 1/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.1484 - acc: 0.9655\n",
            "Epoch 1: loss did not improve from 0.14387\n",
            "126/126 [==============================] - 2s 16ms/step - loss: 0.1485 - acc: 0.9655\n",
            "Epoch 2/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.1443 - acc: 0.9663\n",
            "Epoch 2: loss did not improve from 0.14387\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.1444 - acc: 0.9662\n",
            "Epoch 3/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.1416 - acc: 0.9666\n",
            "Epoch 3: loss improved from 0.14387 to 0.14156, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 17ms/step - loss: 0.1416 - acc: 0.9667\n",
            "Epoch 4/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.1399 - acc: 0.9669\n",
            "Epoch 4: loss improved from 0.14156 to 0.13985, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.1399 - acc: 0.9669\n",
            "Epoch 5/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.1383 - acc: 0.9669\n",
            "Epoch 5: loss improved from 0.13985 to 0.13828, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 16ms/step - loss: 0.1383 - acc: 0.9669\n",
            "Epoch 6/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.1375 - acc: 0.9673\n",
            "Epoch 6: loss improved from 0.13828 to 0.13716, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 16ms/step - loss: 0.1372 - acc: 0.9673\n",
            "Epoch 7/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.1360 - acc: 0.9670\n",
            "Epoch 7: loss improved from 0.13716 to 0.13608, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.1361 - acc: 0.9670\n",
            "Epoch 8/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.1359 - acc: 0.9673\n",
            "Epoch 8: loss improved from 0.13608 to 0.13587, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.1359 - acc: 0.9673\n",
            "Epoch 9/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.1340 - acc: 0.9674\n",
            "Epoch 9: loss improved from 0.13587 to 0.13409, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 16ms/step - loss: 0.1341 - acc: 0.9673\n",
            "Epoch 10/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.1314 - acc: 0.9681\n",
            "Epoch 10: loss improved from 0.13409 to 0.13150, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.1315 - acc: 0.9680\n",
            "Q: 내 가 좋아하는 사람 이 나 좋아해 줬으면 좋겠다\n",
            "A: 저 도 듣고 싶어요 \n",
            "\n",
            "\n",
            "Q: 노는게 제일 좋아\n",
            "A: 자신 의 감정 을 주변 사람 들 에게 터 놓고 이야기 해보세요 \n",
            "\n",
            "\n",
            "Q: 날씨 풀렸다\n",
            "A: 보험 처리 하세요 \n",
            "\n",
            "\n",
            "processing epoch: 191...\n",
            "Epoch 1/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.1284 - acc: 0.9684\n",
            "Epoch 1: loss improved from 0.13150 to 0.12862, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 20ms/step - loss: 0.1286 - acc: 0.9683\n",
            "Epoch 2/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.1274 - acc: 0.9681\n",
            "Epoch 2: loss improved from 0.12862 to 0.12744, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 16ms/step - loss: 0.1274 - acc: 0.9681\n",
            "Epoch 3/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.1254 - acc: 0.9691\n",
            "Epoch 3: loss improved from 0.12744 to 0.12536, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.1254 - acc: 0.9691\n",
            "Epoch 4/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.1226 - acc: 0.9694\n",
            "Epoch 4: loss improved from 0.12536 to 0.12260, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.1226 - acc: 0.9694\n",
            "Epoch 5/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.1228 - acc: 0.9693\n",
            "Epoch 5: loss did not improve from 0.12260\n",
            "126/126 [==============================] - 3s 22ms/step - loss: 0.1228 - acc: 0.9693\n",
            "Epoch 6/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.1223 - acc: 0.9690\n",
            "Epoch 6: loss improved from 0.12260 to 0.12229, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 16ms/step - loss: 0.1223 - acc: 0.9690\n",
            "Epoch 7/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.1183 - acc: 0.9696\n",
            "Epoch 7: loss improved from 0.12229 to 0.11827, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.1183 - acc: 0.9696\n",
            "Epoch 8/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.1166 - acc: 0.9701\n",
            "Epoch 8: loss improved from 0.11827 to 0.11663, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.1166 - acc: 0.9701\n",
            "Epoch 9/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.1130 - acc: 0.9706\n",
            "Epoch 9: loss improved from 0.11663 to 0.11305, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.1130 - acc: 0.9706\n",
            "Epoch 10/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.1098 - acc: 0.9715\n",
            "Epoch 10: loss improved from 0.11305 to 0.11014, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 16ms/step - loss: 0.1101 - acc: 0.9714\n",
            "Q: 나 잘 하는게 없는거 같아\n",
            "A: 저 도 보고 싶어요 \n",
            "\n",
            "\n",
            "Q: 대리 효도 짜증\n",
            "A: 좋은 만남이었길 바라요 \n",
            "\n",
            "\n",
            "Q: 눈 또 와\n",
            "A: 눈 이 많이 내렸나 봐요 \n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "예측"
      ],
      "metadata": {
        "id": "imyCHN6PNigN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 자연어 (질문 입력) 대한 전처리 함수\n",
        "def make_question(sentence):\n",
        "    sentence = clean_and_morph(sentence)\n",
        "    question_sequence = tokenizer.texts_to_sequences([sentence])\n",
        "    question_padded = pad_sequences(question_sequence, maxlen=MAX_LENGTH, truncating='post', padding='post')\n",
        "    return question_padded\n",
        "\n",
        "make_question('오늘 날씨 어때?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTRg9oQsNfhm",
        "outputId": "5dec7909-e0f0-49a4-e310-f40bfa35ad00"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[124, 170, 347,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 챗봇\n",
        "def run_chatbot(question):\n",
        "    question_inputs = make_question(question)\n",
        "    results = make_prediction(seq2seq, question_inputs)\n",
        "    results = convert_index_to_text(results, END_TOKEN)\n",
        "    return results\n",
        "\n",
        "# 챗봇 실행\n",
        "while True:\n",
        "    user_input = input('<< 말을 걸어 보세요!\\n')\n",
        "    if user_input == 'q':\n",
        "        break\n",
        "    print('>> 챗봇 응답: {}'.format(run_chatbot(user_input)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNi6zCnLNknN",
        "outputId": "3c6ec55d-c783-4895-d0aa-632c2aead715"
      },
      "execution_count": 36,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<< 말을 걸어 보세요!\n",
            "안녕\n",
            ">> 챗봇 응답: 약 바르고 얼른 나으세요 \n",
            "<< 말을 걸어 보세요!\n",
            "아니 안녕이라고\n",
            ">> 챗봇 응답: 자신 의 감정 을 주변 사람 들 에게 터 놓고 이야기 해보세요 \n",
            "<< 말을 걸어 보세요!\n",
            "어이없네\n",
            ">> 챗봇 응답: 약 바르고 얼른 나으세요 \n",
            "<< 말을 걸어 보세요!\n",
            "뭐해\n",
            ">> 챗봇 응답: 남 들 눈 은 신경 쓰지 마세요 \n",
            "<< 말을 걸어 보세요!\n",
            "요즘 고민이있어\n",
            ">> 챗봇 응답: 자신 하고 만나는 시간 도 중요해요 \n",
            "<< 말을 걸어 보세요!\n",
            "공부할까?\n",
            ">> 챗봇 응답: 자신 만 행복하면 돼요 \n",
            "<< 말을 걸어 보세요!\n",
            "넌 행복해?\n",
            ">> 챗봇 응답: 자신 만 행복하면 돼요 \n",
            "<< 말을 걸어 보세요!\n",
            "q\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Seq2Seq with Attention**"
      ],
      "metadata": {
        "id": "ft62p9ipNrha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, Attention\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
        "from tensorflow.keras.utils import plot_model"
      ],
      "metadata": {
        "id": "H-88OjrGNnFI"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 인코더\n",
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, units, vocab_size, embedding_dim, time_steps):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = Embedding(vocab_size,\n",
        "                                   embedding_dim,\n",
        "                                   input_length=time_steps,\n",
        "                                   name='Embedding')\n",
        "        self.dropout = Dropout(0.2, name='Dropout')\n",
        "        # (attention) return_sequences=True 추가\n",
        "        self.lstm = LSTM(units,\n",
        "                         return_state=True,\n",
        "                         return_sequences=True,\n",
        "                         name='LSTM')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.embedding(inputs)\n",
        "        x = self.dropout(x)\n",
        "        x, hidden_state, cell_state = self.lstm(x)\n",
        "        # (attention) x return 추가\n",
        "        return x, [hidden_state, cell_state]"
      ],
      "metadata": {
        "id": "ry_XaAE0N9fc"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 디코더\n",
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, units, vocab_size, embedding_dim, time_steps):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = Embedding(vocab_size,\n",
        "                                   embedding_dim,\n",
        "                                   input_length=time_steps,\n",
        "                                   name='Embedding')\n",
        "        self.dropout = Dropout(0.2, name='Dropout')\n",
        "        self.lstm = LSTM(units,\n",
        "                         return_state=True,\n",
        "                         return_sequences=True,\n",
        "                         name='LSTM'\n",
        "                        )\n",
        "        self.attention = Attention(name='Attention')\n",
        "        self.dense = Dense(vocab_size,\n",
        "                           activation='softmax',\n",
        "                           name='Dense')\n",
        "\n",
        "    def call(self, inputs, initial_state):\n",
        "        # (attention) encoder_inputs 추가\n",
        "        encoder_inputs, decoder_inputs = inputs\n",
        "        x = self.embedding(decoder_inputs)\n",
        "        x = self.dropout(x)\n",
        "        x, hidden_state, cell_state = self.lstm(x, initial_state=initial_state)\n",
        "\n",
        "        # (attention) key_value, attention_matrix 추가\n",
        "        # 이전 hidden_state의 값을 concat으로 만들어 vector를 생성합니다.\n",
        "        key_value = tf.concat([initial_state[0][:, tf.newaxis, :],\n",
        "                               x[:, :-1, :]], axis=1)\n",
        "        # 이전 hidden_state의 값을 concat으로 만든 vector와 encoder에서 나온\n",
        "        # 출력 값들로 attention을 구합니다.\n",
        "        attention_matrix = self.attention([key_value, encoder_inputs])\n",
        "        # 위에서 구한 attention_matrix와 decoder의 출력 값을 concat 합니다.\n",
        "        x = tf.concat([x, attention_matrix], axis=-1)\n",
        "\n",
        "        x = self.dense(x)\n",
        "        return x, hidden_state, cell_state"
      ],
      "metadata": {
        "id": "zv70TTHjOJ3s"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2Seq(tf.keras.Model):\n",
        "    def __init__(self, units, vocab_size, embedding_dim, time_steps, start_token, end_token):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.start_token = start_token\n",
        "        self.end_token = end_token\n",
        "        self.time_steps = time_steps\n",
        "\n",
        "        self.encoder = Encoder(units, vocab_size, embedding_dim, time_steps)\n",
        "        self.decoder = Decoder(units, vocab_size, embedding_dim, time_steps)\n",
        "\n",
        "\n",
        "    def call(self, inputs, training=True):\n",
        "        if training:\n",
        "            encoder_inputs, decoder_inputs = inputs\n",
        "            # (attention) encoder 출력 값 수정\n",
        "            encoder_outputs, context_vector = self.encoder(encoder_inputs)\n",
        "            # (attention) decoder 입력 값 수정\n",
        "            decoder_outputs, _, _ = self.decoder((encoder_outputs, decoder_inputs),\n",
        "                                                 initial_state=context_vector)\n",
        "            return decoder_outputs\n",
        "        else:\n",
        "            x = inputs\n",
        "            # (attention) encoder 출력 값 수정\n",
        "            encoder_outputs, context_vector = self.encoder(x)\n",
        "            target_seq = tf.constant([[self.start_token]], dtype=tf.float32)\n",
        "            results = tf.TensorArray(tf.int32, self.time_steps)\n",
        "\n",
        "            for i in tf.range(self.time_steps):\n",
        "                decoder_output, decoder_hidden, decoder_cell = self.decoder((encoder_outputs, target_seq),\n",
        "                                                                            initial_state=context_vector)\n",
        "                decoder_output = tf.cast(tf.argmax(decoder_output, axis=-1), dtype=tf.int32)\n",
        "                decoder_output = tf.reshape(decoder_output, shape=(1, 1))\n",
        "                results = results.write(i, decoder_output)\n",
        "\n",
        "                if decoder_output == self.end_token:\n",
        "                    break\n",
        "\n",
        "                target_seq = decoder_output\n",
        "                context_vector = [decoder_hidden, decoder_cell]\n",
        "\n",
        "            return tf.reshape(results.stack(), shape=(1, self.time_steps))"
      ],
      "metadata": {
        "id": "_EJv088EONzC"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 체크포인트\n",
        "checkpoint_path = 'model/seq2seq-attention-checkpoint.ckpt'\n",
        "checkpoint = ModelCheckpoint(filepath=checkpoint_path,\n",
        "                             save_weights_only=True,\n",
        "                             save_best_only=True,\n",
        "                             monitor='loss',\n",
        "                             verbose=1\n",
        "                            )"
      ],
      "metadata": {
        "id": "aRiLKUupOPsS"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f'processing epoch: {epoch * 10 + 1}...')\n",
        "    seq2seq.fit([question_padded, answer_in_padded],\n",
        "                answer_out_one_hot,\n",
        "                epochs=10,\n",
        "                batch_size=BATCH_SIZE,\n",
        "                callbacks=[checkpoint]\n",
        "               )\n",
        "    # 랜덤한 샘플 번호 추출\n",
        "    samples = np.random.randint(DATA_LENGTH, size=SAMPLE_SIZE)\n",
        "\n",
        "    # 예측 성능 테스트\n",
        "    for idx in samples:\n",
        "        question_inputs = question_padded[idx]\n",
        "        # 문장 예측\n",
        "        results = make_prediction(seq2seq, np.expand_dims(question_inputs, 0))\n",
        "\n",
        "        # 변환된 인덱스를 문장으로 변환\n",
        "        results = convert_index_to_text(results, END_TOKEN)\n",
        "\n",
        "        print(f'Q: {questions[idx]}')\n",
        "        print(f'A: {results}\\n')\n",
        "        print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-w5PaQWOTsH",
        "outputId": "a25f358c-9035-408a-b458-19d4bbaf415f"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processing epoch: 1...\n",
            "Epoch 1/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.1080 - acc: 0.9724\n",
            "Epoch 1: loss improved from inf to 0.10801, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 5s 36ms/step - loss: 0.1080 - acc: 0.9724\n",
            "Epoch 2/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.1076 - acc: 0.9716\n",
            "Epoch 2: loss improved from 0.10801 to 0.10756, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 3s 22ms/step - loss: 0.1076 - acc: 0.9716\n",
            "Epoch 3/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.1055 - acc: 0.9723\n",
            "Epoch 3: loss improved from 0.10756 to 0.10547, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.1055 - acc: 0.9722\n",
            "Epoch 4/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.1033 - acc: 0.9729\n",
            "Epoch 4: loss improved from 0.10547 to 0.10329, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.1033 - acc: 0.9729\n",
            "Epoch 5/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.1020 - acc: 0.9731\n",
            "Epoch 5: loss improved from 0.10329 to 0.10199, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.1020 - acc: 0.9731\n",
            "Epoch 6/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.1002 - acc: 0.9739\n",
            "Epoch 6: loss improved from 0.10199 to 0.10016, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.1002 - acc: 0.9738\n",
            "Epoch 7/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.0982 - acc: 0.9737\n",
            "Epoch 7: loss improved from 0.10016 to 0.09817, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.0982 - acc: 0.9737\n",
            "Epoch 8/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0956 - acc: 0.9747\n",
            "Epoch 8: loss improved from 0.09817 to 0.09554, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0955 - acc: 0.9747\n",
            "Epoch 9/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0931 - acc: 0.9751\n",
            "Epoch 9: loss improved from 0.09554 to 0.09318, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0932 - acc: 0.9751\n",
            "Epoch 10/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0915 - acc: 0.9754\n",
            "Epoch 10: loss improved from 0.09318 to 0.09170, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0917 - acc: 0.9754\n",
            "Q: 기분 이 더러워\n",
            "A: 차근차근 이뤄 보아 요 \n",
            "\n",
            "\n",
            "Q: 기분 이 그 지 같아\n",
            "A: 자신 을 이겨 야해요 \n",
            "\n",
            "\n",
            "Q: 무서운 꿈 꿨어\n",
            "A: 다음 에는 다를거예요 \n",
            "\n",
            "\n",
            "processing epoch: 11...\n",
            "Epoch 1/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0894 - acc: 0.9759\n",
            "Epoch 1: loss improved from 0.09170 to 0.08950, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 16ms/step - loss: 0.0895 - acc: 0.9758\n",
            "Epoch 2/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.0872 - acc: 0.9769\n",
            "Epoch 2: loss improved from 0.08950 to 0.08716, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.0872 - acc: 0.9769\n",
            "Epoch 3/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0860 - acc: 0.9771\n",
            "Epoch 3: loss improved from 0.08716 to 0.08597, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0860 - acc: 0.9771\n",
            "Epoch 4/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0838 - acc: 0.9775\n",
            "Epoch 4: loss improved from 0.08597 to 0.08387, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 16ms/step - loss: 0.0839 - acc: 0.9775\n",
            "Epoch 5/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.0835 - acc: 0.9774\n",
            "Epoch 5: loss improved from 0.08387 to 0.08345, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 19ms/step - loss: 0.0834 - acc: 0.9774\n",
            "Epoch 6/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.0823 - acc: 0.9782\n",
            "Epoch 6: loss improved from 0.08345 to 0.08232, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0823 - acc: 0.9782\n",
            "Epoch 7/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.0787 - acc: 0.9792\n",
            "Epoch 7: loss improved from 0.08232 to 0.07891, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.0789 - acc: 0.9791\n",
            "Epoch 8/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.0796 - acc: 0.9787\n",
            "Epoch 8: loss did not improve from 0.07891\n",
            "126/126 [==============================] - 2s 13ms/step - loss: 0.0796 - acc: 0.9786\n",
            "Epoch 9/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0769 - acc: 0.9800\n",
            "Epoch 9: loss improved from 0.07891 to 0.07707, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0771 - acc: 0.9800\n",
            "Epoch 10/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0738 - acc: 0.9808\n",
            "Epoch 10: loss improved from 0.07707 to 0.07391, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.0739 - acc: 0.9807\n",
            "Q: 노후 걱정 벌써 해야하나\n",
            "A: 노후 는 지금 부터 준비 하는 게 좋죠 \n",
            "\n",
            "\n",
            "Q: 내 스타일 아니던데\n",
            "A: 새로운 스타일 도전 해 보시 면 어때요 \n",
            "\n",
            "\n",
            "Q: 나 는 그냥 저 냥 사는 거 같아\n",
            "A: 오늘 은 바지 가 좋을거 같아요 \n",
            "\n",
            "\n",
            "processing epoch: 21...\n",
            "Epoch 1/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0724 - acc: 0.9812\n",
            "Epoch 1: loss improved from 0.07391 to 0.07267, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 16ms/step - loss: 0.0727 - acc: 0.9810\n",
            "Epoch 2/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0694 - acc: 0.9821\n",
            "Epoch 2: loss improved from 0.07267 to 0.06933, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0693 - acc: 0.9821\n",
            "Epoch 3/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.0685 - acc: 0.9821\n",
            "Epoch 3: loss improved from 0.06933 to 0.06858, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0686 - acc: 0.9820\n",
            "Epoch 4/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.0679 - acc: 0.9821\n",
            "Epoch 4: loss improved from 0.06858 to 0.06792, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0679 - acc: 0.9821\n",
            "Epoch 5/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.0647 - acc: 0.9835\n",
            "Epoch 5: loss improved from 0.06792 to 0.06471, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.0647 - acc: 0.9834\n",
            "Epoch 6/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0643 - acc: 0.9831\n",
            "Epoch 6: loss improved from 0.06471 to 0.06434, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.0643 - acc: 0.9831\n",
            "Epoch 7/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.0602 - acc: 0.9846\n",
            "Epoch 7: loss improved from 0.06434 to 0.06020, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0602 - acc: 0.9846\n",
            "Epoch 8/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0599 - acc: 0.9842\n",
            "Epoch 8: loss improved from 0.06020 to 0.05990, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.0599 - acc: 0.9842\n",
            "Epoch 9/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0577 - acc: 0.9852\n",
            "Epoch 9: loss improved from 0.05990 to 0.05768, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0577 - acc: 0.9852\n",
            "Epoch 10/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0570 - acc: 0.9852\n",
            "Epoch 10: loss improved from 0.05768 to 0.05695, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0569 - acc: 0.9852\n",
            "Q: 말 거 는 게 어려워\n",
            "A: 어울리는 향수 가 있을 거 예요 \n",
            "\n",
            "\n",
            "Q: 바 본가 봐\n",
            "A: 바보 는 자기 한테 바보 라고 하지 않아요 \n",
            "\n",
            "\n",
            "Q: 대학 가는 게 낫 지\n",
            "A: 장단점 을 분석 해보세요 \n",
            "\n",
            "\n",
            "processing epoch: 31...\n",
            "Epoch 1/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0556 - acc: 0.9859\n",
            "Epoch 1: loss improved from 0.05695 to 0.05574, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.0557 - acc: 0.9858\n",
            "Epoch 2/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.0578 - acc: 0.9852\n",
            "Epoch 2: loss did not improve from 0.05574\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0578 - acc: 0.9851\n",
            "Epoch 3/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0657 - acc: 0.9829\n",
            "Epoch 3: loss did not improve from 0.05574\n",
            "126/126 [==============================] - 2s 13ms/step - loss: 0.0655 - acc: 0.9830\n",
            "Epoch 4/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0566 - acc: 0.9858\n",
            "Epoch 4: loss did not improve from 0.05574\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0566 - acc: 0.9858\n",
            "Epoch 5/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.0545 - acc: 0.9857\n",
            "Epoch 5: loss improved from 0.05574 to 0.05462, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0546 - acc: 0.9856\n",
            "Epoch 6/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.0493 - acc: 0.9878\n",
            "Epoch 6: loss improved from 0.05462 to 0.04925, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0492 - acc: 0.9878\n",
            "Epoch 7/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.0481 - acc: 0.9881\n",
            "Epoch 7: loss improved from 0.04925 to 0.04808, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 19ms/step - loss: 0.0481 - acc: 0.9881\n",
            "Epoch 8/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.0471 - acc: 0.9881\n",
            "Epoch 8: loss improved from 0.04808 to 0.04744, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 16ms/step - loss: 0.0474 - acc: 0.9880\n",
            "Epoch 9/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0503 - acc: 0.9868\n",
            "Epoch 9: loss did not improve from 0.04744\n",
            "126/126 [==============================] - 2s 13ms/step - loss: 0.0502 - acc: 0.9868\n",
            "Epoch 10/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.0448 - acc: 0.9888\n",
            "Epoch 10: loss improved from 0.04744 to 0.04476, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0448 - acc: 0.9888\n",
            "Q: 나도 이제 아 재인 가\n",
            "A: 고민 하고 있으면 그럴 거 예요 \n",
            "\n",
            "\n",
            "Q: 도서관 앞 자리 사람 괜찮다\n",
            "A: 스트레스 를 받으시나 봐요 \n",
            "\n",
            "\n",
            "Q: 너 미워\n",
            "A: 모두 제 잘 못 입니다 \n",
            "\n",
            "\n",
            "processing epoch: 41...\n",
            "Epoch 1/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.0427 - acc: 0.9897\n",
            "Epoch 1: loss improved from 0.04476 to 0.04266, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.0427 - acc: 0.9897\n",
            "Epoch 2/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.0428 - acc: 0.9894\n",
            "Epoch 2: loss did not improve from 0.04266\n",
            "126/126 [==============================] - 2s 19ms/step - loss: 0.0428 - acc: 0.9894\n",
            "Epoch 3/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0404 - acc: 0.9901\n",
            "Epoch 3: loss improved from 0.04266 to 0.04047, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 16ms/step - loss: 0.0405 - acc: 0.9900\n",
            "Epoch 4/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0394 - acc: 0.9905\n",
            "Epoch 4: loss improved from 0.04047 to 0.03938, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0394 - acc: 0.9905\n",
            "Epoch 5/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.0384 - acc: 0.9907\n",
            "Epoch 5: loss improved from 0.03938 to 0.03838, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0384 - acc: 0.9907\n",
            "Epoch 6/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0372 - acc: 0.9914\n",
            "Epoch 6: loss improved from 0.03838 to 0.03726, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0373 - acc: 0.9914\n",
            "Epoch 7/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9912\n",
            "Epoch 7: loss improved from 0.03726 to 0.03670, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0367 - acc: 0.9912\n",
            "Epoch 8/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.0359 - acc: 0.9915\n",
            "Epoch 8: loss improved from 0.03670 to 0.03590, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 17ms/step - loss: 0.0359 - acc: 0.9915\n",
            "Epoch 9/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.0357 - acc: 0.9914\n",
            "Epoch 9: loss improved from 0.03590 to 0.03580, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 18ms/step - loss: 0.0358 - acc: 0.9914\n",
            "Epoch 10/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.0347 - acc: 0.9921\n",
            "Epoch 10: loss improved from 0.03580 to 0.03462, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0346 - acc: 0.9921\n",
            "Q: 더우니까 무서운 이야기 듣고싶지\n",
            "A: 해주세요 \n",
            "\n",
            "\n",
            "Q: 다리 떠는거 말 들음\n",
            "A: 남 에게 피해 만 주지 않는다면 괜찮아요 \n",
            "\n",
            "\n",
            "Q: 그런 사람인 가보다 해야하나 봐\n",
            "A: 대인 배시군요 \n",
            "\n",
            "\n",
            "processing epoch: 51...\n",
            "Epoch 1/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.0334 - acc: 0.9926\n",
            "Epoch 1: loss improved from 0.03462 to 0.03343, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0334 - acc: 0.9925\n",
            "Epoch 2/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9918\n",
            "Epoch 2: loss did not improve from 0.03343\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0375 - acc: 0.9918\n",
            "Epoch 3/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.0355 - acc: 0.9920\n",
            "Epoch 3: loss did not improve from 0.03343\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0354 - acc: 0.9920\n",
            "Epoch 4/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.0333 - acc: 0.9923\n",
            "Epoch 4: loss did not improve from 0.03343\n",
            "126/126 [==============================] - 2s 17ms/step - loss: 0.0336 - acc: 0.9921\n",
            "Epoch 5/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0319 - acc: 0.9929\n",
            "Epoch 5: loss improved from 0.03343 to 0.03187, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 16ms/step - loss: 0.0319 - acc: 0.9929\n",
            "Epoch 6/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9936\n",
            "Epoch 6: loss improved from 0.03187 to 0.02949, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0295 - acc: 0.9936\n",
            "Epoch 7/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9927\n",
            "Epoch 7: loss did not improve from 0.02949\n",
            "126/126 [==============================] - 2s 13ms/step - loss: 0.0301 - acc: 0.9927\n",
            "Epoch 8/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9942\n",
            "Epoch 8: loss improved from 0.02949 to 0.02730, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0273 - acc: 0.9942\n",
            "Epoch 9/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9941\n",
            "Epoch 9: loss improved from 0.02730 to 0.02720, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0272 - acc: 0.9941\n",
            "Epoch 10/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0257 - acc: 0.9947\n",
            "Epoch 10: loss improved from 0.02720 to 0.02581, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 17ms/step - loss: 0.0258 - acc: 0.9947\n",
            "Q: 다시 태어나도 챗봇 할래\n",
            "A: 저 는 사람 으로 태어나고 싶어요 \n",
            "\n",
            "\n",
            "Q: 남친 에 내 사진 없어\n",
            "A: 신경 쓰지 마세요 \n",
            "\n",
            "\n",
            "Q: 밥 탔다\n",
            "A: 어흥 호랑이 보다 무섭나요 \n",
            "\n",
            "\n",
            "processing epoch: 61...\n",
            "Epoch 1/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0254 - acc: 0.9949\n",
            "Epoch 1: loss improved from 0.02581 to 0.02530, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.0253 - acc: 0.9948\n",
            "Epoch 2/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0243 - acc: 0.9949\n",
            "Epoch 2: loss improved from 0.02530 to 0.02430, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0243 - acc: 0.9948\n",
            "Epoch 3/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.0242 - acc: 0.9951\n",
            "Epoch 3: loss improved from 0.02430 to 0.02418, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0242 - acc: 0.9950\n",
            "Epoch 4/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.0235 - acc: 0.9950\n",
            "Epoch 4: loss improved from 0.02418 to 0.02347, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 17ms/step - loss: 0.0235 - acc: 0.9950\n",
            "Epoch 5/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0219 - acc: 0.9956\n",
            "Epoch 5: loss improved from 0.02347 to 0.02185, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 19ms/step - loss: 0.0219 - acc: 0.9956\n",
            "Epoch 6/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.0209 - acc: 0.9959\n",
            "Epoch 6: loss improved from 0.02185 to 0.02101, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.0210 - acc: 0.9959\n",
            "Epoch 7/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0206 - acc: 0.9955\n",
            "Epoch 7: loss improved from 0.02101 to 0.02069, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.0207 - acc: 0.9956\n",
            "Epoch 8/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0216 - acc: 0.9958\n",
            "Epoch 8: loss did not improve from 0.02069\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0216 - acc: 0.9958\n",
            "Epoch 9/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0217 - acc: 0.9957\n",
            "Epoch 9: loss did not improve from 0.02069\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0219 - acc: 0.9957\n",
            "Epoch 10/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.0206 - acc: 0.9960\n",
            "Epoch 10: loss improved from 0.02069 to 0.02061, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0206 - acc: 0.9960\n",
            "Q: 내 가 나빴네\n",
            "A: 아니에요 너무 자책 하지 마세요 \n",
            "\n",
            "\n",
            "Q: 내 가 무능력하게 느껴져\n",
            "A: 잘 할 수 있는 게 다른 거 예요 \n",
            "\n",
            "\n",
            "Q: 나 한테 만은 완전 솔직했으면\n",
            "A: 믿음 이 가장 중요하죠 \n",
            "\n",
            "\n",
            "processing epoch: 71...\n",
            "Epoch 1/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0201 - acc: 0.9961\n",
            "Epoch 1: loss improved from 0.02061 to 0.02008, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 18ms/step - loss: 0.0201 - acc: 0.9961\n",
            "Epoch 2/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.0181 - acc: 0.9962\n",
            "Epoch 2: loss improved from 0.02008 to 0.01807, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.0181 - acc: 0.9962\n",
            "Epoch 3/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0175 - acc: 0.9964\n",
            "Epoch 3: loss improved from 0.01807 to 0.01746, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.0175 - acc: 0.9964\n",
            "Epoch 4/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0175 - acc: 0.9968\n",
            "Epoch 4: loss did not improve from 0.01746\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0175 - acc: 0.9968\n",
            "Epoch 5/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.0188 - acc: 0.9961\n",
            "Epoch 5: loss did not improve from 0.01746\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0187 - acc: 0.9961\n",
            "Epoch 6/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.0170 - acc: 0.9967\n",
            "Epoch 6: loss improved from 0.01746 to 0.01697, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.0170 - acc: 0.9967\n",
            "Epoch 7/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.0156 - acc: 0.9972\n",
            "Epoch 7: loss improved from 0.01697 to 0.01567, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.0157 - acc: 0.9972\n",
            "Epoch 8/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.9968\n",
            "Epoch 8: loss did not improve from 0.01567\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0165 - acc: 0.9968\n",
            "Epoch 9/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0157 - acc: 0.9969\n",
            "Epoch 9: loss did not improve from 0.01567\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0157 - acc: 0.9969\n",
            "Epoch 10/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.0157 - acc: 0.9971\n",
            "Epoch 10: loss did not improve from 0.01567\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0157 - acc: 0.9972\n",
            "Q: 공무원 준비 할까\n",
            "A: 시작 이 반 이에요 어서 준비 하세요 \n",
            "\n",
            "\n",
            "Q: 감기 기운 이 있어\n",
            "A: 이럴 때 잘 쉬는 게 중요해요 \n",
            "\n",
            "\n",
            "Q: 꿈 이 너무 무서웠어\n",
            "A: 요즘 예민한 가봐요 \n",
            "\n",
            "\n",
            "processing epoch: 81...\n",
            "Epoch 1/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.0154 - acc: 0.9970\n",
            "Epoch 1: loss improved from 0.01567 to 0.01544, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.0154 - acc: 0.9970\n",
            "Epoch 2/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0148 - acc: 0.9975\n",
            "Epoch 2: loss improved from 0.01544 to 0.01488, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 17ms/step - loss: 0.0149 - acc: 0.9975\n",
            "Epoch 3/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0193 - acc: 0.9959\n",
            "Epoch 3: loss did not improve from 0.01488\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0192 - acc: 0.9959\n",
            "Epoch 4/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.0148 - acc: 0.9972\n",
            "Epoch 4: loss improved from 0.01488 to 0.01480, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0148 - acc: 0.9972\n",
            "Epoch 5/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0126 - acc: 0.9978\n",
            "Epoch 5: loss improved from 0.01480 to 0.01277, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0128 - acc: 0.9977\n",
            "Epoch 6/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.0135 - acc: 0.9974\n",
            "Epoch 6: loss did not improve from 0.01277\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0135 - acc: 0.9975\n",
            "Epoch 7/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.0124 - acc: 0.9978\n",
            "Epoch 7: loss improved from 0.01277 to 0.01236, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0124 - acc: 0.9978\n",
            "Epoch 8/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.0130 - acc: 0.9974\n",
            "Epoch 8: loss did not improve from 0.01236\n",
            "126/126 [==============================] - 2s 19ms/step - loss: 0.0130 - acc: 0.9974\n",
            "Epoch 9/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0115 - acc: 0.9979\n",
            "Epoch 9: loss improved from 0.01236 to 0.01153, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 16ms/step - loss: 0.0115 - acc: 0.9979\n",
            "Epoch 10/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0110 - acc: 0.9981\n",
            "Epoch 10: loss improved from 0.01153 to 0.01097, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0110 - acc: 0.9982\n",
            "Q: 먹기 도 귀찮아\n",
            "A: 영상 을 보며 연습 해봅시다 \n",
            "\n",
            "\n",
            "Q: 명품 선물 부담스러울까\n",
            "A: 고가 의 선물 은 부담스러울 수도 있어요 \n",
            "\n",
            "\n",
            "Q: 노래 잘 하는 사람 부러워\n",
            "A: 저 도 부러워요 \n",
            "\n",
            "\n",
            "processing epoch: 91...\n",
            "Epoch 1/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0107 - acc: 0.9983\n",
            "Epoch 1: loss improved from 0.01097 to 0.01071, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0107 - acc: 0.9983\n",
            "Epoch 2/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0114 - acc: 0.9979\n",
            "Epoch 2: loss did not improve from 0.01071\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0115 - acc: 0.9979\n",
            "Epoch 3/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.0407 - acc: 0.9925\n",
            "Epoch 3: loss did not improve from 0.01071\n",
            "126/126 [==============================] - 2s 16ms/step - loss: 0.0405 - acc: 0.9925\n",
            "Epoch 4/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0434 - acc: 0.9906\n",
            "Epoch 4: loss did not improve from 0.01071\n",
            "126/126 [==============================] - 2s 18ms/step - loss: 0.0440 - acc: 0.9905\n",
            "Epoch 5/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0576 - acc: 0.9865\n",
            "Epoch 5: loss did not improve from 0.01071\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0576 - acc: 0.9865\n",
            "Epoch 6/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.0411 - acc: 0.9902\n",
            "Epoch 6: loss did not improve from 0.01071\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0410 - acc: 0.9902\n",
            "Epoch 7/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0312 - acc: 0.9920\n",
            "Epoch 7: loss did not improve from 0.01071\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0312 - acc: 0.9920\n",
            "Epoch 8/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.0245 - acc: 0.9942\n",
            "Epoch 8: loss did not improve from 0.01071\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0245 - acc: 0.9942\n",
            "Epoch 9/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.0211 - acc: 0.9952\n",
            "Epoch 9: loss did not improve from 0.01071\n",
            "126/126 [==============================] - 2s 16ms/step - loss: 0.0211 - acc: 0.9952\n",
            "Epoch 10/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0192 - acc: 0.9957\n",
            "Epoch 10: loss did not improve from 0.01071\n",
            "126/126 [==============================] - 2s 18ms/step - loss: 0.0192 - acc: 0.9957\n",
            "Q: 고기 먹고 싶어\n",
            "A: 저기압 에는 고 기 앞 이 죠 \n",
            "\n",
            "\n",
            "Q: 그냥 택시 타야지\n",
            "A: 조심 히 오세요 \n",
            "\n",
            "\n",
            "Q: 꽃바구니 선물 이랑 과일 바구니 선물 뭐 가 좋아\n",
            "A: 받는 사람 이 부럽네요 \n",
            "\n",
            "\n",
            "processing epoch: 101...\n",
            "Epoch 1/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.0176 - acc: 0.9961\n",
            "Epoch 1: loss did not improve from 0.01071\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.0177 - acc: 0.9961\n",
            "Epoch 2/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0179 - acc: 0.9960\n",
            "Epoch 2: loss did not improve from 0.01071\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0179 - acc: 0.9960\n",
            "Epoch 3/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0161 - acc: 0.9968\n",
            "Epoch 3: loss did not improve from 0.01071\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0161 - acc: 0.9968\n",
            "Epoch 4/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0145 - acc: 0.9973\n",
            "Epoch 4: loss did not improve from 0.01071\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0145 - acc: 0.9973\n",
            "Epoch 5/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0136 - acc: 0.9973\n",
            "Epoch 5: loss did not improve from 0.01071\n",
            "126/126 [==============================] - 2s 19ms/step - loss: 0.0136 - acc: 0.9973\n",
            "Epoch 6/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.0131 - acc: 0.9975\n",
            "Epoch 6: loss did not improve from 0.01071\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.0133 - acc: 0.9974\n",
            "Epoch 7/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.0128 - acc: 0.9974\n",
            "Epoch 7: loss did not improve from 0.01071\n",
            "126/126 [==============================] - 2s 16ms/step - loss: 0.0128 - acc: 0.9974\n",
            "Epoch 8/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0128 - acc: 0.9975\n",
            "Epoch 8: loss did not improve from 0.01071\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0127 - acc: 0.9975\n",
            "Epoch 9/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.0134 - acc: 0.9974\n",
            "Epoch 9: loss did not improve from 0.01071\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0134 - acc: 0.9974\n",
            "Epoch 10/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0116 - acc: 0.9977\n",
            "Epoch 10: loss did not improve from 0.01071\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0116 - acc: 0.9977\n",
            "Q: 밥 먹었니\n",
            "A: 배고프지 않아요 \n",
            "\n",
            "\n",
            "Q: 발표 할 때 자꾸 떨려\n",
            "A: 발표 는 누구 나 떨려요 \n",
            "\n",
            "\n",
            "Q: 나 누구 게\n",
            "A: 저 도 궁금하네요 \n",
            "\n",
            "\n",
            "processing epoch: 111...\n",
            "Epoch 1/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0104 - acc: 0.9981\n",
            "Epoch 1: loss improved from 0.01071 to 0.01036, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.0104 - acc: 0.9981\n",
            "Epoch 2/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0135 - acc: 0.9970\n",
            "Epoch 2: loss did not improve from 0.01036\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0137 - acc: 0.9970\n",
            "Epoch 3/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.0115 - acc: 0.9977\n",
            "Epoch 3: loss did not improve from 0.01036\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0115 - acc: 0.9977\n",
            "Epoch 4/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.0107 - acc: 0.9979\n",
            "Epoch 4: loss did not improve from 0.01036\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0107 - acc: 0.9979\n",
            "Epoch 5/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.0098 - acc: 0.9980\n",
            "Epoch 5: loss improved from 0.01036 to 0.00974, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0097 - acc: 0.9980\n",
            "Epoch 6/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.0092 - acc: 0.9982\n",
            "Epoch 6: loss improved from 0.00974 to 0.00917, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 19ms/step - loss: 0.0092 - acc: 0.9982\n",
            "Epoch 7/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.0089 - acc: 0.9984\n",
            "Epoch 7: loss improved from 0.00917 to 0.00888, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 16ms/step - loss: 0.0089 - acc: 0.9984\n",
            "Epoch 8/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0088 - acc: 0.9983\n",
            "Epoch 8: loss improved from 0.00888 to 0.00883, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 16ms/step - loss: 0.0088 - acc: 0.9983\n",
            "Epoch 9/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.0088 - acc: 0.9984\n",
            "Epoch 9: loss improved from 0.00883 to 0.00881, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0088 - acc: 0.9984\n",
            "Epoch 10/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9985\n",
            "Epoch 10: loss improved from 0.00881 to 0.00803, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0080 - acc: 0.9985\n",
            "Q: 내 가 만족 을 못 해\n",
            "A: 스스로 좋다고 못 느끼는게 제일 어려운 것 같아요 \n",
            "\n",
            "\n",
            "Q: 무슨 매주 결혼식 이야\n",
            "A: 인맥 이 넓으신 가봐요 \n",
            "\n",
            "\n",
            "Q: 골프 어려워\n",
            "A: 너무 집착 하지 마세요 \n",
            "\n",
            "\n",
            "processing epoch: 121...\n",
            "Epoch 1/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0071 - acc: 0.9988\n",
            "Epoch 1: loss improved from 0.00803 to 0.00711, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 18ms/step - loss: 0.0071 - acc: 0.9988\n",
            "Epoch 2/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.0077 - acc: 0.9986\n",
            "Epoch 2: loss did not improve from 0.00711\n",
            "126/126 [==============================] - 2s 18ms/step - loss: 0.0077 - acc: 0.9985\n",
            "Epoch 3/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.0075 - acc: 0.9987\n",
            "Epoch 3: loss did not improve from 0.00711\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0075 - acc: 0.9987\n",
            "Epoch 4/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.0083 - acc: 0.9983\n",
            "Epoch 4: loss did not improve from 0.00711\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0083 - acc: 0.9984\n",
            "Epoch 5/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.0091 - acc: 0.9982\n",
            "Epoch 5: loss did not improve from 0.00711\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0091 - acc: 0.9982\n",
            "Epoch 6/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0115 - acc: 0.9976\n",
            "Epoch 6: loss did not improve from 0.00711\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0115 - acc: 0.9976\n",
            "Epoch 7/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0088 - acc: 0.9984\n",
            "Epoch 7: loss did not improve from 0.00711\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0088 - acc: 0.9984\n",
            "Epoch 8/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0087 - acc: 0.9981\n",
            "Epoch 8: loss did not improve from 0.00711\n",
            "126/126 [==============================] - 2s 18ms/step - loss: 0.0087 - acc: 0.9981\n",
            "Epoch 9/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.0091 - acc: 0.9981\n",
            "Epoch 9: loss did not improve from 0.00711\n",
            "126/126 [==============================] - 2s 17ms/step - loss: 0.0091 - acc: 0.9981\n",
            "Epoch 10/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0081 - acc: 0.9983\n",
            "Epoch 10: loss did not improve from 0.00711\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0081 - acc: 0.9983\n",
            "Q: 나 은근 무시 하는 애 있어\n",
            "A: 콕 집어서 물어보세요 \n",
            "\n",
            "\n",
            "Q: 목 이 칼칼해\n",
            "A: 목 감기 오려나 봐요 \n",
            "\n",
            "\n",
            "Q: 개 당황\n",
            "A: 다음 부터는 더 많이 아세요 \n",
            "\n",
            "\n",
            "processing epoch: 131...\n",
            "Epoch 1/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.0081 - acc: 0.9985\n",
            "Epoch 1: loss did not improve from 0.00711\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.0080 - acc: 0.9985\n",
            "Epoch 2/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0072 - acc: 0.9986\n",
            "Epoch 2: loss did not improve from 0.00711\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0072 - acc: 0.9987\n",
            "Epoch 3/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.0064 - acc: 0.9990\n",
            "Epoch 3: loss improved from 0.00711 to 0.00641, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 18ms/step - loss: 0.0064 - acc: 0.9990\n",
            "Epoch 4/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.0080 - acc: 0.9984\n",
            "Epoch 4: loss did not improve from 0.00641\n",
            "126/126 [==============================] - 2s 17ms/step - loss: 0.0080 - acc: 0.9984\n",
            "Epoch 5/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.0077 - acc: 0.9985\n",
            "Epoch 5: loss did not improve from 0.00641\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0077 - acc: 0.9985\n",
            "Epoch 6/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0092 - acc: 0.9980\n",
            "Epoch 6: loss did not improve from 0.00641\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0092 - acc: 0.9980\n",
            "Epoch 7/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.0123 - acc: 0.9969\n",
            "Epoch 7: loss did not improve from 0.00641\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0122 - acc: 0.9969\n",
            "Epoch 8/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0086 - acc: 0.9982\n",
            "Epoch 8: loss did not improve from 0.00641\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.0086 - acc: 0.9982\n",
            "Epoch 9/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9986\n",
            "Epoch 9: loss did not improve from 0.00641\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0079 - acc: 0.9986\n",
            "Epoch 10/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0075 - acc: 0.9986\n",
            "Epoch 10: loss did not improve from 0.00641\n",
            "126/126 [==============================] - 2s 18ms/step - loss: 0.0075 - acc: 0.9986\n",
            "Q: 내 가 기대 를 너무 많이 했나 봐\n",
            "A: 아무 것 도 바라지 않을 때 천하 를 얻는다는 말 이 있어요 \n",
            "\n",
            "\n",
            "Q: 낙엽 밟는 소리\n",
            "A: 가을 이네 요 \n",
            "\n",
            "\n",
            "Q: 내 잘못 이 뭔 지 모르겠어\n",
            "A: 모르는 게 잘못 인 거 같아요 \n",
            "\n",
            "\n",
            "processing epoch: 141...\n",
            "Epoch 1/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9990\n",
            "Epoch 1: loss improved from 0.00641 to 0.00632, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 19ms/step - loss: 0.0063 - acc: 0.9990\n",
            "Epoch 2/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.0054 - acc: 0.9991\n",
            "Epoch 2: loss improved from 0.00632 to 0.00539, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.0054 - acc: 0.9991\n",
            "Epoch 3/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.0057 - acc: 0.9991\n",
            "Epoch 3: loss did not improve from 0.00539\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0057 - acc: 0.9991\n",
            "Epoch 4/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0062 - acc: 0.9987\n",
            "Epoch 4: loss did not improve from 0.00539\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.0061 - acc: 0.9987\n",
            "Epoch 5/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.0065 - acc: 0.9987\n",
            "Epoch 5: loss did not improve from 0.00539\n",
            "126/126 [==============================] - 2s 18ms/step - loss: 0.0065 - acc: 0.9987\n",
            "Epoch 6/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.0054 - acc: 0.9991\n",
            "Epoch 6: loss did not improve from 0.00539\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.0055 - acc: 0.9991\n",
            "Epoch 7/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.0050 - acc: 0.9990\n",
            "Epoch 7: loss improved from 0.00539 to 0.00500, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.0050 - acc: 0.9990\n",
            "Epoch 8/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.0046 - acc: 0.9993\n",
            "Epoch 8: loss improved from 0.00500 to 0.00460, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.0046 - acc: 0.9993\n",
            "Epoch 9/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0043 - acc: 0.9993\n",
            "Epoch 9: loss improved from 0.00460 to 0.00428, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.0043 - acc: 0.9993\n",
            "Epoch 10/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.0131 - acc: 0.9972\n",
            "Epoch 10: loss did not improve from 0.00428\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0131 - acc: 0.9972\n",
            "Q: 마음 에 하나 도 안 들어\n",
            "A: 그럴 때 가 있죠 \n",
            "\n",
            "\n",
            "Q: 다리 떨면 안 좋아\n",
            "A: 의식 하는 다리 떨기 는 괜찮 대요 \n",
            "\n",
            "\n",
            "Q: 딸기 뷔페 가고 싶어\n",
            "A: 저 도 딸기 좋아해요 같이 가요 \n",
            "\n",
            "\n",
            "processing epoch: 151...\n",
            "Epoch 1/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.9982\n",
            "Epoch 1: loss did not improve from 0.00428\n",
            "126/126 [==============================] - 2s 18ms/step - loss: 0.0082 - acc: 0.9982\n",
            "Epoch 2/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.0134 - acc: 0.9969\n",
            "Epoch 2: loss did not improve from 0.00428\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0134 - acc: 0.9969\n",
            "Epoch 3/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9984\n",
            "Epoch 3: loss did not improve from 0.00428\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0076 - acc: 0.9983\n",
            "Epoch 4/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.0068 - acc: 0.9987\n",
            "Epoch 4: loss did not improve from 0.00428\n",
            "126/126 [==============================] - 2s 16ms/step - loss: 0.0068 - acc: 0.9987\n",
            "Epoch 5/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0071 - acc: 0.9985\n",
            "Epoch 5: loss did not improve from 0.00428\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0071 - acc: 0.9985\n",
            "Epoch 6/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.0067 - acc: 0.9987\n",
            "Epoch 6: loss did not improve from 0.00428\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.0067 - acc: 0.9987\n",
            "Epoch 7/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.0083 - acc: 0.9984\n",
            "Epoch 7: loss did not improve from 0.00428\n",
            "126/126 [==============================] - 2s 19ms/step - loss: 0.0083 - acc: 0.9984\n",
            "Epoch 8/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0092 - acc: 0.9980\n",
            "Epoch 8: loss did not improve from 0.00428\n",
            "126/126 [==============================] - 2s 16ms/step - loss: 0.0091 - acc: 0.9981\n",
            "Epoch 9/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.0064 - acc: 0.9987\n",
            "Epoch 9: loss did not improve from 0.00428\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0064 - acc: 0.9987\n",
            "Epoch 10/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0055 - acc: 0.9990\n",
            "Epoch 10: loss did not improve from 0.00428\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0055 - acc: 0.9990\n",
            "Q: 대출 언제 다 갚냐\n",
            "A: 조금씩 갚아나가세요 \n",
            "\n",
            "\n",
            "Q: 말 이 제일 무서운 듯\n",
            "A: 다른 사람 말 은 한 귀로 흘리세요 \n",
            "\n",
            "\n",
            "Q: 날씨 건조한 거 같 애\n",
            "A: 미스트 나 가습기 젖은 수건 등 을 사용 해보세요 \n",
            "\n",
            "\n",
            "processing epoch: 161...\n",
            "Epoch 1/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0055 - acc: 0.9989\n",
            "Epoch 1: loss did not improve from 0.00428\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.0055 - acc: 0.9989\n",
            "Epoch 2/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0052 - acc: 0.9989\n",
            "Epoch 2: loss did not improve from 0.00428\n",
            "126/126 [==============================] - 2s 18ms/step - loss: 0.0052 - acc: 0.9989\n",
            "Epoch 3/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.0049 - acc: 0.9990\n",
            "Epoch 3: loss did not improve from 0.00428\n",
            "126/126 [==============================] - 2s 17ms/step - loss: 0.0049 - acc: 0.9991\n",
            "Epoch 4/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0050 - acc: 0.9991\n",
            "Epoch 4: loss did not improve from 0.00428\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0051 - acc: 0.9990\n",
            "Epoch 5/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0041 - acc: 0.9991\n",
            "Epoch 5: loss improved from 0.00428 to 0.00418, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.0042 - acc: 0.9991\n",
            "Epoch 6/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.0152 - acc: 0.9974\n",
            "Epoch 6: loss did not improve from 0.00418\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0151 - acc: 0.9973\n",
            "Epoch 7/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9956\n",
            "Epoch 7: loss did not improve from 0.00418\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0280 - acc: 0.9956\n",
            "Epoch 8/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.0079 - acc: 0.9981\n",
            "Epoch 8: loss did not improve from 0.00418\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.0079 - acc: 0.9981\n",
            "Epoch 9/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.0054 - acc: 0.9987\n",
            "Epoch 9: loss did not improve from 0.00418\n",
            "126/126 [==============================] - 2s 19ms/step - loss: 0.0054 - acc: 0.9987\n",
            "Epoch 10/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.0054 - acc: 0.9987\n",
            "Epoch 10: loss did not improve from 0.00418\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.0055 - acc: 0.9987\n",
            "Q: 그렇게 갈 거 면서\n",
            "A: 이야기 를 해보세요 \n",
            "\n",
            "\n",
            "Q: 나른하다\n",
            "A: 아무 것 도 안해 도 괜찮아요 \n",
            "\n",
            "\n",
            "Q: 그냥 이렇게 살 고 싶어\n",
            "A: 살 고 싶은대로 사세요 \n",
            "\n",
            "\n",
            "processing epoch: 171...\n",
            "Epoch 1/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0050 - acc: 0.9988\n",
            "Epoch 1: loss did not improve from 0.00418\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.0050 - acc: 0.9988\n",
            "Epoch 2/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0039 - acc: 0.9993\n",
            "Epoch 2: loss improved from 0.00418 to 0.00399, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 18ms/step - loss: 0.0040 - acc: 0.9993\n",
            "Epoch 3/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0049 - acc: 0.9989\n",
            "Epoch 3: loss did not improve from 0.00399\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.0049 - acc: 0.9989\n",
            "Epoch 4/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.0037 - acc: 0.9992\n",
            "Epoch 4: loss improved from 0.00399 to 0.00364, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.0036 - acc: 0.9992\n",
            "Epoch 5/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0043 - acc: 0.9990\n",
            "Epoch 5: loss did not improve from 0.00364\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.0042 - acc: 0.9990\n",
            "Epoch 6/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 0.9990\n",
            "Epoch 6: loss did not improve from 0.00364\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0045 - acc: 0.9990\n",
            "Epoch 7/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0047 - acc: 0.9992\n",
            "Epoch 7: loss did not improve from 0.00364\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0047 - acc: 0.9992\n",
            "Epoch 8/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9994\n",
            "Epoch 8: loss improved from 0.00364 to 0.00340, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.0034 - acc: 0.9994\n",
            "Epoch 9/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0037 - acc: 0.9992\n",
            "Epoch 9: loss did not improve from 0.00340\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0037 - acc: 0.9991\n",
            "Epoch 10/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0040 - acc: 0.9991\n",
            "Epoch 10: loss did not improve from 0.00340\n",
            "126/126 [==============================] - 2s 16ms/step - loss: 0.0040 - acc: 0.9991\n",
            "Q: 맞팔 왜 안 하지 ㅠㅠ\n",
            "A: 잘 모르고 있을 수도 있어요 \n",
            "\n",
            "\n",
            "Q: 건강 빨리 회복해야지\n",
            "A: 세상 의 무엇 보다 건강 이 제일 중요해요 \n",
            "\n",
            "\n",
            "Q: 눈 이 피곤해\n",
            "A: 눈 체조 를 해보세요 \n",
            "\n",
            "\n",
            "processing epoch: 181...\n",
            "Epoch 1/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.0038 - acc: 0.9993\n",
            "Epoch 1: loss did not improve from 0.00340\n",
            "126/126 [==============================] - 2s 19ms/step - loss: 0.0038 - acc: 0.9993\n",
            "Epoch 2/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0046 - acc: 0.9989\n",
            "Epoch 2: loss did not improve from 0.00340\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.0045 - acc: 0.9989\n",
            "Epoch 3/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0038 - acc: 0.9991\n",
            "Epoch 3: loss did not improve from 0.00340\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0038 - acc: 0.9992\n",
            "Epoch 4/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0038 - acc: 0.9993\n",
            "Epoch 4: loss did not improve from 0.00340\n",
            "126/126 [==============================] - 2s 16ms/step - loss: 0.0038 - acc: 0.9993\n",
            "Epoch 5/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.0039 - acc: 0.9992\n",
            "Epoch 5: loss did not improve from 0.00340\n",
            "126/126 [==============================] - 2s 17ms/step - loss: 0.0039 - acc: 0.9992\n",
            "Epoch 6/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9993\n",
            "Epoch 6: loss did not improve from 0.00340\n",
            "126/126 [==============================] - 2s 19ms/step - loss: 0.0034 - acc: 0.9993\n",
            "Epoch 7/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9995\n",
            "Epoch 7: loss improved from 0.00340 to 0.00271, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "126/126 [==============================] - 2s 18ms/step - loss: 0.0027 - acc: 0.9995\n",
            "Epoch 8/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0035 - acc: 0.9992\n",
            "Epoch 8: loss did not improve from 0.00271\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0035 - acc: 0.9992\n",
            "Epoch 9/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0041 - acc: 0.9990\n",
            "Epoch 9: loss did not improve from 0.00271\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.0041 - acc: 0.9990\n",
            "Epoch 10/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.0039 - acc: 0.9991\n",
            "Epoch 10: loss did not improve from 0.00271\n",
            "126/126 [==============================] - 2s 16ms/step - loss: 0.0039 - acc: 0.9991\n",
            "Q: 기획사 에 예쁜 애 들 많겠지\n",
            "A: 연예인 을 준비 하니 일반인 보다 다 예쁘겠죠 \n",
            "\n",
            "\n",
            "Q: 동호회 나가지 말까\n",
            "A: 취미 생활 은 좋은 거 예요 \n",
            "\n",
            "\n",
            "Q: 마음 에 안 드는 짓 만 해\n",
            "A: 너무 미워하지 마세요 \n",
            "\n",
            "\n",
            "processing epoch: 191...\n",
            "Epoch 1/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0039 - acc: 0.9991\n",
            "Epoch 1: loss did not improve from 0.00271\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.0039 - acc: 0.9991\n",
            "Epoch 2/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9983\n",
            "Epoch 2: loss did not improve from 0.00271\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.0067 - acc: 0.9983\n",
            "Epoch 3/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9981\n",
            "Epoch 3: loss did not improve from 0.00271\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0068 - acc: 0.9981\n",
            "Epoch 4/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0057 - acc: 0.9985\n",
            "Epoch 4: loss did not improve from 0.00271\n",
            "126/126 [==============================] - 2s 16ms/step - loss: 0.0057 - acc: 0.9985\n",
            "Epoch 5/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.0049 - acc: 0.9987\n",
            "Epoch 5: loss did not improve from 0.00271\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.0049 - acc: 0.9987\n",
            "Epoch 6/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 0.9989\n",
            "Epoch 6: loss did not improve from 0.00271\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0044 - acc: 0.9989\n",
            "Epoch 7/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.0037 - acc: 0.9992\n",
            "Epoch 7: loss did not improve from 0.00271\n",
            "126/126 [==============================] - 2s 16ms/step - loss: 0.0037 - acc: 0.9992\n",
            "Epoch 8/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.0046 - acc: 0.9990\n",
            "Epoch 8: loss did not improve from 0.00271\n",
            "126/126 [==============================] - 2s 19ms/step - loss: 0.0045 - acc: 0.9990\n",
            "Epoch 9/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.0039 - acc: 0.9991\n",
            "Epoch 9: loss did not improve from 0.00271\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 0.0039 - acc: 0.9991\n",
            "Epoch 10/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.0035 - acc: 0.9991\n",
            "Epoch 10: loss did not improve from 0.00271\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0035 - acc: 0.9991\n",
            "Q: 노래방 가고 싶어\n",
            "A: 즐거운 시간 이 될 거 같아요 \n",
            "\n",
            "\n",
            "Q: 뭘 해도 재미없다\n",
            "A: 활기찬 사람 을 만나 보시 면 생각 이 바뀔 수도 있어요 \n",
            "\n",
            "\n",
            "Q: 뭐 하면서 놀까\n",
            "A: 나 랑 같이 놀아요 \n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    user_input = input('<< 말을 걸어 보세요!\\n')\n",
        "    if user_input == 'q':\n",
        "        break\n",
        "    print('>> 챗봇 응답: {}'.format(run_chatbot(user_input)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BsTdk_spOWUN",
        "outputId": "c37e309b-2eb9-4a99-b53d-d05e5b440daa"
      },
      "execution_count": 42,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<< 말을 걸어 보세요!\n",
            "안녕\n",
            ">> 챗봇 응답: 독서 와 음악 감상 이라고 하고 싶지만 아무 것 도 안 했어요 \n",
            "<< 말을 걸어 보세요!\n",
            "아니 안녕이라고\n",
            ">> 챗봇 응답: 화장실 가세 요 \n",
            "<< 말을 걸어 보세요!\n",
            "오늘 너무 행복해\n",
            ">> 챗봇 응답: 즐거운 시작 되길 바랍니다 \n",
            "<< 말을 걸어 보세요!\n",
            "조금 심심해\n",
            ">> 챗봇 응답: 친구 들 과 연락 해보세요 \n",
            "<< 말을 걸어 보세요!\n",
            "배고픈데 뭐 먹을까?\n",
            ">> 챗봇 응답: 좀 먹어도 괜찮아요 \n",
            "<< 말을 걸어 보세요!\n",
            "아니 메뉴 추천좀 해줘\n",
            ">> 챗봇 응답: 독서 와 음악 감상 이라고 하고 싶지만 아무 것 도 안 했어요 \n",
            "<< 말을 걸어 보세요!\n",
            "메뉴 추천 해줘\n",
            ">> 챗봇 응답: 내 집 마련 축하 드려요 \n",
            "<< 말을 걸어 보세요!\n",
            "나 집 없어...\n",
            ">> 챗봇 응답: 냉장고 파먹기 해보세요 \n",
            "<< 말을 걸어 보세요!\n",
            "q\n"
          ]
        }
      ]
    }
  ]
}